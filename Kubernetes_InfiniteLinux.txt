1.Introduction												[28-MAR-2021]  ()
2.Setup MiniKube                                            [31-MAR-2021]  (Done)
3.Kube API Server                                           [31-MAR-2021]  (Done)
4.ETCD in Kubernetes                                        [01-APR-2021]  (Done)
5.Kubernetes Certified Exam                                 [01-APR-2021]  ()
6.Kube Controller Manager and Scheduler                     [01-APR-2021]  (Done)
7.Pods                                                      [12-APR-2021]  (Done)
8.Pod Networking                                            [12-APR-2021]  (Done)
9.Static Pods                                               [27-APR-2021]  ()
10.Container Probes                                         [27-APR-2021]  ()

11.Replica Sets                                             [27-APR-2021]  (Done)
12.Deployment in Kubernetes                                 [28-APR-2021]  (Done)
13.Kubernetes Services Part 1                               [28-APR-2021]  (Done)
14.Kubernetes Services Part 2                               [29-APR-2021]  ()
15.Kubernetes Namespace                                     [29-APR-2021]  (Done)

16.Manual Scheduling                                        [05-MAY-2021]  ()
17.Kubernetes Affinity and Anti Affinity                    [05-MAY-2021]  ()
18.Kubernetes Taints and Tolerants                          [06-MAY-2021]  ()
19.ConfigMaps                                               [06-MAY-2021]  ()
20.Persistent Volumes and Claims                            [06-MAY-2021]  ()
21.ETCD Backup                                              [07-MAY-2021]  ()
22.ETCD Database Restore                                    [07-MAY-2021]  ()
23.Nginx Ingress Controller                                 [07-MAY-2021]  ()
24.Helm Introduction
25.Helm Charts
26.Helm hooks
27.Network policies
28.Service Accounts
29.Read only access
30.Kubernetes Resources
31.Argocd in Kubernetes
32.Pod Topology Spread Constraints
33.Headless Services 

Pod Lifecycle
Services
Scheduling
Label Nodes

************************************************************* 2.Setup MiniKube  *************************************************************

To install the Kubernetes we use the command

	minikube start

*************************************************************  3.Kube API Server  ***********************************************************

API Server is responsible for all the communication on the Kubernetes.
Whenever we are running any kubectl command we are actually intercating with API Serevr.
API Server exposes some API that a user can interact with.we can also intercat with API server using curl Request.
Consider the scenario of creating a Pod and the steps are 

create the yml file
apply

Here request goes to the API Server, creates the Pod, updates the ETCD and shows the user that Pod is created.
The Scheduler which is running in the Kubernetes Cluster continuously monitors the API Server for the newly created Pods.
Whenever Scheduler finds any newly created Pod, It looks for the Worker Node where the Pod has to be placed.

Scheduler gets all the configuration information of Worker Nodes from configuration files i.e. indirectly from ETCD data store.
Whenever Scheduler has scheduled a Pod,it will check which Worker Node is suitable to schedule a Pod based on hardware configuration of a Node.

Scheduler then transfers the information to the API Server.
API Server again updates the ETCD with the Worker Node information.
API Server then sends the information to the kubelet which is running on the worker Node.
kubelet gets the Pod specification from the API Server and communicates with the Container Runtime to create the Pod.
Container Runtime actually creates the Pod and updates the kubelet and then kubelet then updates the API Server that the Pod has created.
API Server then updates the ETCD about the Pod creation.

ETCD is the place where we have complete information about the Cluster.

If we install the Kubernetes as Hardware,download the binary and run the API Server as a Service.
If we install the Kubernetes using kube admin,kube admin runs the API Server as Pod.

Note:
Switch to the console and we can see how the API Server is running as Pod and we can also view all the Configuration details.

To view all the Nodes in the Cluster,we use the command

	kubectl get nodes

To view all the Pods in the Cluster,we use the command

	kubectl get pods

Here no Pods are running as of now.
Here we are not able to see all the Kubernetes Components as Pods because all these Pods are deployed in different Namespaces.
All these system Pods runs in a namespace called kube-system.

	kubectl get pods --namespace kube-system (or) kubectl get pods -n kube-system

To see the configuration of the API Server Pod,we use the command

	kubectl describe pod kube-apiserver-minikube -n kube-system

If we want to see the response in Json format

	kubectl get pods -o json

Note :
------
To run the kubectl commnads in docker desktop, we need to enable Kubernetes in Docker desktop settings and then apply.


*************************************************************  4.ETCD in Kubernetes  ***************************************************************

ETCD is an open source,distributed,consistent and highly available key value data store that can be used to store all the cluster data in the in Kubernetes.
ETCD is a single source of truth for all the components of the Kubernetes Cluster.
ETCD stores everything that is happening in the Kubernetes Cluster i.e Adding Node,Pod Creation,Any Deployment etc.
Whenever we do migration in Kubernetes,ETCD is the most importnat Component and If we loose the ETCD we loose the data.

ETCD has consistent read and write i.e Any data which we write that is immideately available for read.
ETCD is highly available because data is replicated in all the Nodes.
ETCD is a Cluster of its own.It can be a 3 Node or 5 Node Cluster and maintain a leadership among them using RAFT protocol.
ETCD is secure.

If we set up the Kubernetes cluster from scratch,we can isolate ETCD cluster by creating as a seperate Cluster of its own.
Then advertise the address to the Kubernetes Cluster.
If we set up the Kubernetes cluster using kubeadm,kubeadm deploys ETCD as Pod.

Note :
In production Environment we normally see the ETCD as an external Component and not deployed along with the Kubernetes Cluster.

To start the kubernetes we use the command

	minikube start

To view all the Pods in the Kubernetes system,we use the command

	kubectl get pods -n kube-system

To view the Configurations of the ETCD Pod we use the command

	kubectl describe pod etcd-minikube -n kube-system

If we install the Kubernetes as a Hardware download the binary and run the ETCD as a Service and runs on port 2379.
All the configuration information will go inside the system d service file.

To intercat with ETCD Cluster or ETCD Pod which is deployed by the minikube local system

	kubectl exec etcd-minikube -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints https// --cacert --key --cert get / -prefix --keys-only"

To insert the data into the ETCD

	kubectl exec etcd-minikube -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints https// --cacert --key --cert put name bhaumik"

	kubectl exec etcd-minikube -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints https// --cacert --key --cert get name"


************************************************************* 6.Kube Controller Manager and Scheduler  *************************************

Controller Manager :
--------------------
Control Manager is the component of the Master Node that runs different Managers.we have 

KUBE-CONTROLLER-MANAGER
CLOUD-CONTROLLER-MANAGER

All these controllers are seperate process and however to reduce the complexity they are compiled into a single binary and run as a single process.

If we set up the Kubernetes cluster from scratch,download the binary from Kubernetes release page and run the ControllerManager as a Service.
If we set up the Kubernetes cluster using kubeadm then ControllerManager will be running as a Pod.

Node Controller is responsible for monitoring the Nodes and whenever a Node goes down it will send a signal.
Replication Controller ensures the number of Pods are running as per the Configuration ie keeps monitoring the API Server for any changes.
When particular Pod goes down,kubelet shares the information to the API Server which then shares the information to the ETCD.
Controller Manager gets the information about the Pod from the API Server and 
If the Pod is down then the Replication Controller tries to restart the Pod in the same Node or another Node.

Scheduler :
-----------
Scheduler is a Component of Master Node which schedules the pod across multiple Worker Nodes.
Scheduler watches the newly created pod that has no Worker Node assigned to it and selects a Worker Node for them to run on.
Here Worker Nodes can be a physical Machine or Virtual Machine.
We can have different infrastructure or Hardware configuration for the Worker Nodes.
Scheduler gets all the information about the Worker Nodes from Configuration Files i.e indirectly from ETCD data store.
Whenever Scheduler has scheduled a Pod,it will check which Worker Node will best fit to schedule a Pod based on Hardware Configuration of a Node.

Scheduler has two stages that can be used to deploy the Pod in a correct Node.
1.Filtering
2.Scoring

Consider the scenario we have 4 Worker Nodes.

Woker Node 1  : 1 GB
Woker Node 2  : 500 MB
Woker Node 3  : 10 GB
Woker Node 4  : 20 GB

If we want to deploy a Pod which requires 2GB of Memory.
Here Scheduler filters all the worker Nodes and  gets the suitable Nodes.
Scheduler then scores the Nodes based on the available resources and places the Pod into the Node which has the highest score.

To view the Pods in the kube-system namespace we use the command

	kubectl get pods -n kube-system

To describe the configuration of the controller manager

	kubectl describe pod kube-controller-manager-minikube -n kube-system

To describe the configuration of the Scheduler

	kubectl describe pod kube-scheduler-minikube -n kube-system

Note :
We can also create custom scheduler when we craete the Pod.


************************************************************* 7.Pods  ***************************************************************

Pod is the smallest deployable unit in Kubernetes.
Pod hosts the Containers i.e. Pod can contain multiple Containers.
All the Containers within the Pod shares the same Network namespace and Storage namespace and can communicate with each other using localhost.
It is the Pod that get's the IP Address to communicate and not the Container.

To run the Pod using imperative command

	kubectl run nginx --image=nginx 
	
To see the list of Pods 

	kubetcl get pods
	
To delete the Pod we use the command

	kubectl delete pod nginx
	
To get the yaml file that can be used to create the Pod

	kubectl run nginx --image=nginx --dry-run=client -o yaml -> yamlfromimage.yaml (or)  kubectl run nginx --image=nginx --dry-run=true -o yaml>yamlfromimage.yaml

--dry-run=client flag can be used to ignore the run command.

To run the Pod which has defined using yaml file 

	kubectl apply -f yamlfromimage.yaml
	
Note :
we never run an isolated Pod in Kubernetes environment.We run the Pod as a part of Deployment,ReplicaSet etc.

Static Pods (Not Completed):
----------------------------
These are the Pods which are created on Worker Nodes managed by Master Node and not created using yaml file.
These are the Pods which are managed by kubelet which is running on the worker Node.

Pod Lifecycle (Not Completed):
------------------------------
Pending	
Running  	 		
Succeded  	

    
************************************************************* 8.Pod Networking  *************************************************************

lets consider how the Docker Networking works.
Whenever Docker is installed,default Bridge network named bridge is created.
This default Bridge network gets an IP Address and all the newly created Containers are attached to the default Bridge network.

Whenever we create the Pod all the Containers within the Pod shares the same Network namespace and can communicate with each other using localhost.
In Kubernetes one Pod can reach out to another Pod using IP Address.

Consider the Scenario where the Kubernetes Cluster has 2 Nodes and they are on the Network and can communicate with each other either by using Router or API Gateway.
Each Node has a corresponding IP Address.

When we set up the Kubernetes Cluster,Networking PlugIns are not installed and it informs what kind of networking structure it expects.
We need to set up the Networking PlugIns outside the Cluster.
If the Network PlugIns are not installed then there will be no next hop and ends up in creating the same IP for all the Nodes.
There is no mechanism to tell how to send the packet from one Node to another Node.
Here Network PlugIn comes into the picture.

Functionality of Network PlugIn is to run the Overlay Network across all the Nodes.
Network PlugIn covers the Bridge Netwrok and assigns the IP Address to all the Nodes within the Bridge Network.
Containers running inside the Pod will get the Sequential IP Address.

************************************************************* 9.Static Pods ********************************************************************* 



   
************************************************************* 10.Container Probes  **************************************************************




************************************************************* 11.Replica Sets  ******************************************************************

Replica Set or Replication Controller ensures that the number of Pods are running as per the Configuration.
Replica Set gaurantees the availability of Pod at any given point of time.

Note :
We never run the pods behind the Replica Set directly.
There is a high level feature called Deployment which provides lots more feature than Replica Set.
Deployment actually creates the Replica Set.
we cannot generate the yaml file for ReplicaSet using kubectl command like we created the yaml file for pods.
Create a deployment manifest file and run it as a Replica Set.

	kubectl create deployment myreplica --image=nginx --dry-run=true -o yaml >rs.yaml
	
Output of yaml file :

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myreplica
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myreplica
  strategy: {}
  template:
    metadata:
      labels:
        app: myreplica
    spec:
      containers:
      - image: nginx
        name: nginx
		

Latest Version :
----------------		
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: myreplica
  name: myreplica
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myreplica
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: myreplica
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}


To run the deployment we use the command

	kubectl apply -f rs.yaml
	
To view the ReplicaSet

	kubectl get rs
	
To view the Pods

	kubectl get pods

Note :
If we define a pod named myreplica then that pod will be automatically acquired by ReplicaSet
and it would be terminated because number of Replica's is already defined in ReplicaSet.


************************************************************* 12.Deployment in Kubernetes   ****************************************************

Deployment can be used to deploy the Application in Kubernetes.
If we want to deploy any Application in Kubernetes,we need to deploy them using Deployment.
It is similar to Replica Set with additional features like updating the image of an Application,RollOut's and Rollback of an Application.
To create the deployment manifest file in an imperative way we use the commad 

	kubectl create deployment deploy --image=nginx --replicas=1 --dry-run=client -o yaml > deploy.yaml
	
To run the Deployment we use the command

	kubectl apply -f deploy.yaml
	
Note :
When we craete the deployment,ReplicaSet will be created Automatically.

	kubectl get rs or kubectl get Replica Set
	
To scale the Deployment using imperative command

	kubectl scale deployment deploy --replicas=3
	
Note:
When we perform scaling through imperative command then there will be no change in yaml file.

To change the image in the deployment we use the command

	kubectl set image deployment/deploy nginx=nginx:1.16.1
	
To view the the updated image in the deployment we use the command

	kubectl describe deployment deploy
	
Note :
If we don't specify any strategy then it is Rolling Update i.e terminates the Pod and creates the New Pod
To view the rolling update we use the command

	watch kubectl get pods
	
To check the Rollout status of the deployment we use the command

	kubectl rollout status deployment deploy
	
To check the rollout history of the deployment we use the command

	kubectl rollout history deployment deploy
	
If we want to see what happened in revision 3 we use the command

	kubectl rollout history deployment deploy --revision=3
	
To rollback to the previous revision we use the command

	kubectl rollout undo deployment deploy
	
To rollback to the specific revision
	
	kubectl rollout undo deployment deploy --to-revision=3
	

************************************************************* 13.Kubernetes Services Part 1  *****************************************************

We have seen how pods communicate with each other using IP Address.
This is not useful in all the scenario's.

Consider the scenario we have 2 Pods.
Application Pod
Database Pod

	Application Pod  --> Database Pod

Pods in Kubernetes are ephimeral and any time a new pod can come which has new Ip Address.
Problem to the solution is we need to use DNS of database instead of Ip Address and any IP Address can be beyond the DNS.
Service in Kubernetes provides the proxy i.e behind the service we can put the pods.

	Aplication Pod  -->  Service  <--  Database Pod

Here Service reverse proxy the Database Pod and holds the IP Address.IP that the service gets is the virtual IP.
Now the Application Pod can use these IP Address to connect to the Database Pod.

	Application Pod  --> Service   <-- Database Pod
	
When the request goes to the service from an Application Pod there is component calle kube-proxy.
kube-proxy maintains all the network configuration and rules.
Whenever request originates for the Ip,it redirects the request to the Node which has the running pods or live pods.

Communication Between Pods :
----------------------------
For the communication to happen between the pods the flow will be
Each node will get the unique Ip Address and there should be an overlay network between the nodes ie Bridge Network.
Bridge Network connects to the Ethernet of the Nodes and it internally connects to the Gateway.
Gateway then communicates with the Ethernet of another Node and which connects to the Bridge Network and the connects to the virtual ethernet of the Pod.

Note :
------
The service which we have created does not have any physical or virtual interface.
The Ip Address which the Service gets is not from or not in the range of Node Network or Pod Network.
This actually has a different IP Address range.

There are 3 types of services and by default is Cluster IP.

Node Port
Cluster IP
Load Balancer

Example on Pod to Pod Communication : (Not Completed)
Prerequisite is Master and Worker Node


************************************************************* 14.Kubernetes Service Part 2   *******************************************************

Pods communication through Service :(Not Completed)
Prerequisite is Master and Worker Node


************************************************************* 15.Kubernetes Namespace **************************************************************

Namespace can be used for resource seggregation i.e running the Pods and Services on one Single Cluster based on different Namespace.
If we need Performance Cluseter and Development Cluster then it is not required to create two Cluster's by spending lot of Money.
Same Cluster can be used as Performance Cluster and Development Cluster by using Namespace.

Namespace in Kubernetes is similar to Namespace in Linux.
There will be Storage Namespace,Netwrok Namespace etc.Similarly we have Namespace for Pods in Kubernetes.
To view the Namespace in Kubernetes we use the command

	kubectl get namespaces
	
If we create any resource then by default it comes under default namespace.

	kubectl get pods -n default
	
To create the namespaces we use the command

	kubectl create namespace development
	kubectl create namespace production
	
To create the Pod in the development namespace we use the command

	kubectl run webserver --image=nginx --namespace=development

Note :
Services which are in one namespace can communicate with the services which are in another namespace by using custom DNS.
Kubernetes internally uses DNS to provide communication between the Services.


************************************************************* 16.Manual Scheduling    *************************************************************




************************************************************* 17.Kubernetes Affinity and Anti Affinity ********************************************