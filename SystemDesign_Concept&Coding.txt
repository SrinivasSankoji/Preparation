1.Introduction											[18-MAY-2024]  ()
2.Network Protocols                                     [18-MAY-2024]  ()
3.CAP Theorem                                           [18-MAY-2024]  ()
4.Decomposition Desgn Pattern 							[18-MAY-2024]  ()
5.SAGA Design,Strangler and CQRS Design Pattren       	[18-MAY-2024]  ()
6.Scale to 1 Million Users                              [18-MAY-2024]  (Read)
7.Consistent Hashing                                    [18-MAY-2024]  (Read)
8.URL Shortner Design Pattern                           [18-MAY-2024]  ()
9.Envelope System Design                                [18-MAY-2024]  ()
10.Key value store                                      [18-MAY-2024]  ()
11.SQL vs NO SQL Differences                            [18-MAY-2024]  ()
12.Whats App System Design                              [18-MAY-2024]  ()
13.Rate Limiter Design Pattern                          [18-MAY-2024]  (Done)
14.Zoom System Design                                   [18-MAY-2024]  ()
15.Idempotent Post API Design Pattern                   [18-MAY-2024]  ()
16.High availability Data Resilient System              [28-JUN-2024]  (Done)
17.Distributed Messaging Queue                          [18-MAY-2024]  ()
18.Proxy vs Reverse Proxy                               [27-MAY-2024]  (Done)
19.Load Balancer                                        [22-JUN-2024]  (Done)
20.Distributed Caching                                  [18-MAY-2024]  (Read)
21.Distributed Transaction                              [18-MAY-2024]  ()
22.Database Indexing                                    [18-MAY-2024]  (Read)
23.Concurrency Control                                  [18-MAY-2024]  ()
24.Two phase Locking                                    [18-MAY-2024]  ()
25.OAuth2                                               [18-MAY-2024]  ()
26.Symmetric and Assymetric Encryption                  [18-MAY-2024]  ()
27.JWT													[18-MAY-2024]  ()


######################################################### 1.Decomposition Desgn Pattern #########################################################



######################################################### 4.Scale to 1 Million Users ############################################################

Load Balancer
Database Replication
CQRS
Cache
CDN
Data Center
Mesagaing Queues
Publisher Subscriber
Rabit MQ
Database Scaling
Database Sharding
Normalization
DeNormalization
Hashing
Consistent Hashing

######################################################### 13.Rate Limiter Design Pattern #######################################################

Consider the scenario of DDOS(Distributed Denial of Service) attack.
In case of DDOS, attacker sends the unwanted requests/huge no of requests to the Server.
Each Server has limited resources and it will consume all the server resources.
When a new request comes the request will be declined as there is no space for the new request.
To prevent this we use Rate Limiter.
There are five different algorithms to implement the Rate Limiter.

1.Token Bucket
2.Leaking Bucket
3.Fixed Window Counter
4.Sliding Window Log
5.Sliding Window Counter

1.Token Bucket :
----------------
In Token Bucket Algorithm there will be a Bucket with certain capacity to hold the Tokens.
Consider the scenario where we have Token Bucket with the capacity 4 i.e. It will hold 4 Tokens.If we try to add more Token it will overflow.
There is something called refiller.
Refiller is nothing but in specified minutes how many Tokens we can add to the Token Bucket.
In Token Bucket algorithm everything i.e. Capacity,refill etc is done by using configuration.

Whenever any new request comes it will validate is there any Token present within the Token Bucket(Capacity).
If the Token is present in the Token Bucket then the request consumes the Token and the capacity will be reduced in the Token Bucket.
If there is no Token present in the Token Bucket then the request will be denied.

we can cofigure the Token Bucket per user.
For each user there will be 3 Tokens per minute to POST an API.

User --> POST request --> Counter : 2 , Time 04:00:10 (Within a Minute and Refiller not started)
User --> POST request --> Counter : 1 , Time 04:00:30 (Within a Minute and Refiller not started)
User --> POST request --> Counter : 0 , Time 04:00:45 (Within a Minute and Refiller not started)
User --> POST request --> Counter : --> request declined sayiing 429 status code i.e. Excess Requests in a Time Limit.

If the request comes at 04:03:45 then refiller tries to add 2 times i.e. from 04:01 to 04:03 but the capacity is limited to 3 then Token Bucket capacity will be 3.
Then it will Post the request by decrementing the counter to 2.

2.Leaking Bucket :
------------------


 




######################################################### 16.High availability Data Resilient System ###########################################

Resilience is Microservices refers to the ability of the system to handle and recover from failures.
Resilience can be asked in another way like How will you design a system with 99% availability or How will you handle Single Point of Failure.
Resilience w.r.t data has two architectural styles i.e. Active-Passive Architecture or Active-Active Architecture.

Single Node :
-------------
With the Single Node Architecture we cannot achieve high available data resilience system.
Consider the scenario where we have the client with the Load Balancer which will route to different server's.
Here all the Servers are connected to the Single Database.
The problem with this approach is if the Database goes down entire Application will goes down.
Database does not have the capability to recover from the failure.

MultiNode :
-----------
MultiNode has two types of architectures.

1.Active-Passive :
------------------
Every organization has atleast two Data Centers based on region.
Consider the scenario where we have a Client with the Load Balancer.
Load Balancer will route the request to nearest Data Center i.e. DC1 or DC2.
In each Data Center multiple instances of an Application will be running to handle the request.
Also multiple instances are connected to the Database.

In Active-Passive architecture out of multiple Data Centers one Data Center will be Primary Data Center and other will be Disaster Recovery Data Center.
The Primary Data Center Database is Primary/Live Database which will be used for Read/Write Operations.
The Disaster Recovery Data Center Database will be Replica Database.

Consider the scenario of Write request and through Load Balancer request came to DC1 which is primary.
Then one of the Server process the request and saves the data into Primary Database.
If one more request came for Read and through Load Balancer request came to DC1 which is primary.
Then one of the Server process the request and fetched the data into Primary Database and send it as a response.
Similary one more request came for Write and through Load Balancer request came to DC2 which is Disaster Recovery Data Center.
Then one of the Server process the request and saves the data into Primary Database instead of Replica Database.
This is nothing but Active-Passive architecture.
In this case there will be a sync up between Primary Database to Replica Database.

From the above scenario the Replica Database is not utilized.
The reason is some of the Databases like Oracle/MySql/Postgres are not Multi Mastered.
This means which can write into one Primary Database and read from Replica Database.
Replica Database are also known as Read Only Database.

Advantages :
------------
If the Primary Database goes down then one of the Replica Database will act as a Primary Database.

Disadvantages :
---------------
1.If the request came to DC2 through Load Balancer and some latency is there in storing the data to Primary Database.
If another request came for Read then the updated data will not be available.
2.If the Primary Database goes down and there are some of the Write Requests at the same time.
In this case will lose the Write Requests.

2.Active-Active :
-----------------
Most of the NoSQL Databases support Multi Master.

Consider the scenario of Write request and through Load Balancer request came to DC1 which is Primary.
Then one of the Server process the request and saves the data into Primary Database.
If one more request came for Read and through Load Balancer request came to DC1 which is primary.
Then one of the Server process the request and fetched the data into Primary Database and send it as a response.
Similary one more request came for Write and through Load Balancer request came to DC2 which is also Primary Database.
Then one of the Server process the request and saves the data into DC2  Primary Database.
This is nothing but Active-Active architecture.
In this case there will be a bi-directional sync up between Primary Database in DC1 to Primary Database in DC2.

Advantages :
------------
Here we are completely utilizing the Server resources in all the Data Centers.
Can handle more traffic as it will take care of both Read and Write.

Disadvantages :
---------------
The problem with the above approach is sync up between the both Primary Databases.
1.Consider the scenario where the Write request came DC1 and Read request for DC2 then it won't have the updated data.
2.If there is an Update request for both DC1 and DC2 then there will be a conflict.


######################################################### 18.Proxy vs Reverse Proxy  ###########################################################

Proxy Server is as an intermediatory server which acts as a Gateway between Client and Server.
All the requests which are coming from the Client are handled by Proxy Server and Proxy Server forwards the request to the required Server.
Proxy Server can take the requests from multiple Clients.
Here no Client and Server can communicate directly with each other.

There are two types of Proxy Servers.
1.Forward Proxy
2.Reverse Proxy

1.Forward Proxy :
-----------------
Consider the scenario of closed Network i.e. Intranet where we have group of Computers.
These group of Computers Communicates with the Forward Proxy.
Proxy Server communicates with the Internet outside world and reaches to the particular Server and sends the response back to the Client.
Forward Proxy sits between Client and the Internet.
Forward Proxy hides the Client Network to the Server.
Server does not know the information about the Client as the request is made from Proxy Server.

Advantages :
------------
1.Forward Proxy provides Anonymity by hiding the Client's IPAddress.
2.Forward Proxy can group the similar requests and send it as a Single Request.
3.Forward Proxy allows the users to access region restricted content by bypassing internet censorship.
Consider the scenario where the Client is is in India and the Client makes a request to the website which is in UK.
Through proxy Server we can call the website and get the data.
4.Forward Proxy provides the additional layer of security by restricting the Clinet to provide the access to certain content.
5.Forward proxy helps the organization to enforce internet usage policies and monitor user activities.
6.Forward Proxy provides additional layer of security by filtering out malicious content and protects the internal network from external threats.
7.Forward proxy provides the Caching by storing the frequently requested data.

Disadvantages :
---------------
1.Forward Proxy is a single point of failure.If the Proxy Server goes down then Client may loose the Internet.
2.By adding an Extra Server in the communication path can increase the latency and slows down the internet.
3.Setting up the Proxy Server and maintaining the Proxy Server requires additional resources and is cost effective.
4.Forward Proxy Server works on the Application Layer.If we have 10 applications then we need to set up the 10 different Proxy Server.

2.Reverse Proxy :
-----------------
In case of Reverse Proxy, Proxy Server sits in between the Internet and the List of Servers.
The request which is coming from Internet cannot access the Server directly.
Proxy Server will send the request to the appropriate Server.

Advantages :
------------
Reverse Proxy will provide the security by hiding the Server's IPAddress.
Attacker can only attack the Reverse Proxy Server.
Some of the Reverse Proxy servers are CDN, Nginx etc.
Reverse proxy provides the Caching.
Reverse Proxy provides the latency and Load Balancing.

Proxy vs VPN :
--------------

Proxy vs Load Balancer :
------------------------

Proxy vs Firewall :
-------------------


######################################################### 19.Load Balancer  ####################################################################

When a request comes from Client to the Server, All the request should not go to one single Server.
This will overload the Server.
We will keep the Load balancer to handle the requests from the Client that will point to different Servers.
The main purpose of the Load Balancer is to distribute the traffic to the appropriate Server so that none of the Server will get overloaded.

There are two types of Load Balancer types.

1.Network Load Balancer Type (L4)
2.Application Load Balancer Type (L7)

Application Load Balancer can read the Header,Request Data,Session and even Response also.
Application Load Balancer can take decission to distribute the traffic based on the information present in Header,Request Data,Session,Cookie etc.
Application Load Balancer also has the capability of doing the caching because it can read the response.
Application Load Balancer is more advanced.

Network Load Balancer cn have TCP port,UDP port,IP Address of source and Destination etc.
Based on these information only Network Load Balancer can take the decission to route the request to appropriate Server.
Network Load Balancer is faster.

There are different type of Algorithms used by Load Balancer to handle the requests.

Static Algorithms :
1.Round Robin Algorithm
2.Weighted Round Robin Algorithm
3.IP Hash

Static Load Balancer algorithms distribute the request to the Server without taking into account Servers real time conditions and performance metrics.
Main advantage is simplicity.

Dynamic Algorithms :
Least Connection
Weighted Least Connection
Least Response Time

Round Robin Algorithm :
-----------------------
Consider the scenario of 4 User Requests and have two Servers.
Here Load Balancer will distribute the traffic to the Server based on Round Robin manner.

Advantages :
It is very easy to implement.
Equal Load Distribution to all the Servers.

Disadvantages:
One Server with high capacity and another Server with Low capacity will be treated as the same.
There is chance that Low capacity Server will get slow down because of the overload of requests.

Weighted Round Robin Algorithm :
--------------------------------
In case of Weighted Round Robin Algorithm, Load Balancer considers the capacity of the Server and based on the capacity 
It will distribute the request to the Server.
Consider the Server1 has 3 times capacity then Server2.
Consider the scenario of 4 User Requests and have two Servers.
Here Request 1,2,3 goes to Server1 and Request 4 goes to Server2.

Advantages :
Low capacity Server will get saved from recieving Large number of Requests.
Easy to implement as the weights are static and no dynamic computation.
When any New Server is getting added, Will provide the weight to the Server.
Load Balancer konows the Weight of the newly added Server.

Disadvantages :
If the request has different processing time then it is possible that Low Capacity Server
will get high processing requests and get over burdened.
Because some of the requests will take more time to process the request.
In that case Low Capacity Server will contains the requests which are in processing and new Requests will get added.

IP Hash :
---------
Consider the scanario of Client Sending the Request to the Server.
Each Client has an IPAddress.
Load Balancer will use the IPAddress to compute the hash.
Using the hash function will get some value and based on the value Load Balancer will assign the request to the Server.

Advantages :
It is good for the Usecase where same Client needs to conenct to Same Server.
It is easy to implement.

Disadvantages :
If the Client request is coming from Proxy then all the Clients will have teh same IPAddress then it will overload the Server.
One more disadvantage is it cannot ensure equal distribution.

Least Connection :
------------------
Dynamically computed to determine the Load Balancing.
Load Balancer will check the number of active connections on each Server.
The server which has the less active connections then Load Balancer will route the request to the particular Server.

Advantages :
Considers the Load on each Server.
There is no chance of burden on the Server which has equal capacity.

Disadvantages :
Consider the case of TCP connections.
Here always it will be considered as active but there is a possibility of no traffic.
Consider the scenario of TCP connection where the Request1 and Request2 are processed by Server1.
Where as Request3 which has huge traffic from Request3 which is processed by Server2 because Server1 has 2 active connections.
Here Low Capacity Server will get over burdened.

Weighted Least Connection :
---------------------------
In this case Server will be assigned with the weight.
Consider the scenario where the Server1 has 10 times capacity of Server2.
Also the Server has the information of active connections.
When a new Request comes Load Balancer will calculate the ratio of no of active connections to its weight.

Sever1 has 2 active connections i.e 2/10 = 0.5
Server2 has 1 active connections  i.e 1/1= 1
Then Load Balancer will transfer the request to Server2 which has minimum ratio.

Least Response Time :
---------------------
TTFB stands for Time to First Byte.
The Time Interval between sending a request and recieving the response from the Server.

Load Balancer will forward the request to the Server which has least Active COnnection and least TTFB.

	least actve connection * TTFB
	
In case if anny clash happens then it follows the Round Robin.


#########################################################  20.Distributed Caching  ######################################################### 


                                    