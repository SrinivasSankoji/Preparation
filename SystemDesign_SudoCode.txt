1.Introduction											[22-JUN-2024]  ()
2.Components of System Design                           [22-JUN-2024]  ()
3.Client Server Architecture                            [22-JUN-2024]  ()
4.Proxies                                               [22-JUN-2024]  ()
5.Data and Data flow                                    [22-JUN-2024]  ()
6.Database Types                                        [22-JUN-2024]  ()
7.Anatomy of Application Services                       [22-JUN-2024]  ()
8.API                                                   [22-JUN-2024]  ()
9.Caching Patterns                                      [22-JUN-2024]  (Done)
10.Rest API                                             [22-JUN-2024]  ()
11.Message Queues                                       [22-JUN-2024]  ()
12.Publisher Subscriber                                 [22-JUN-2024]  ()
13.Publisher Subscriber Use Cases                       [22-JUN-2024]  ()
14.Performance Metrics                                  [22-JUN-2024]  ()
15.Performance Metrics of Components                    [22-JUN-2024]  ()
16.Fault and Failure in Distributed System              [22-JUN-2024]  ()
17.Scaling                                              [22-JUN-2024]  ()
18.Database Replication                                 [22-JUN-2024]  (Read)
19.CAP                                                  [22-JUN-2024]  ()
20.CAP Theorem                                          [22-JUN-2024]  ()
21.Database Sharding                                    [23-JUN-2024]  (Done)
22.Key based Sharding                                   [22-JUN-2024]  ()
23.Range based Sharding                                 [22-JUN-2024]  ()
24.Directory based Sharding                             [22-JUN-2024]  ()
25.Hashing                                              [24-JUN-2024]  (Done)
26.Consistent Hashing                                   [22-JUN-2024]  (Read)
27.Capacity Estimation                                  [22-JUN-2024]  ()

############################################################### 1.Introduction ##########################################################################



############################################################### 9.Caching Patterns ######################################################################

https://dzone.com/articles/introducing-amp-assimilating-caching-quick-read-fo

Cache is defined as a Hardware or Software Component which helps in serving the data which is frequently requested.
The data which is not changed frequently can be stored in the Cache to improove the performance of the System.

Consider the scenario where the Client is requesting the images from the Server.
Server has to fetch the images from Disc/Database and sent it as a response to the Client.
If multiple Clients are requesting for the same image,fetching the the images from Disc/Database is an expensive operation.

To avoid this we have two solutions.
1.we can Cache or Copy the image in the Server Side i.e. by using Reverse Proxy.
2.Cache the copy of an image at the Client side i.e. Browser Cache.

Consider the scenario of fetching the data from the Database.
If the same request is repeated then fetching the data from Database is redundant.
This can be avoided by keeping the request and response in the Cache.
Next time when the Request comes it will check in the Cache whether the requested data is present or not.
If it is present (Cache Hit) it will fetch the data from the Cache otherwise it is Cache Miss. Then it will fetch the data from the Database.

Cache Invalidation :
--------------------
The data which is kept in the Cache is going to change at some point of time.
If the data got changed then the data in the Cache has to be invalidated or removed from the Cache.
This is called Cache invalidation.

Consider the case of users in the Security Expert System.
There are multiple ways to invalidate the Cache.

1.Cache Expiry Time :
Cache Expiry Time can be done by using TTL (Time to Live).
In this case against each entry will mention the expiry time.
Disadvantage is if we keep the low TTL then evry time it will hit the Database and get the data from Database and keep it in Cache.
In this scenario we are not utilizing the Cache feature.
If we keep the TTL high then we will not get the updated data.
The above scenario will be resolved based on the business requirement.

2.There are some other methods which also invalidate the Cache other than Cache Expiry Time.
Whenever the Cient requests for the new data then Cache Miss will happen and Application will fetch the data from Database. 
Application then removes the data from the Cache and places the new data.
Application can also update the data in the Cache instead of replacing it with the new data.
In this case Cache Miss will not happen.

Cache Eviction :
----------------
Consider the scenario where even after applying expiry time there are many keys in the Cache.
Every Cache solution has a limit to store the keys in the Cache.
Consider the scenario where the Cache limit is 1000 and already there are 1000 keys in the Cache.
If a new key has to be placed in the Cache then any of the existing key in the Cache has to removed.
This is called Cache Eviction.
There are multiple ways through which Cache Eviction is implemented.

1.First In First Out 	(FIFO)
2.Least Recently Used 	(LRU)
3.Least Frequently Used (LFU)

Different type of Cache providers implement different Eviction strategies depending on the usecase.

Caching strategies :
--------------------
There are different Caching strategies to implement the Caching.

1.Cache aside
2.Read Through
3.Write Through
4.Write Around
5.Write Back

1.Cache aside :
---------------
In Cache aside pattern Cache always communicatees with the Application Server.
In this case Cache is kept a side and never communicates with Database.
The problem with this approach is when the Database is updated then the Cache has to be invalidated or updated with the data.

Advantages :
Even if the Cache is not available then the Application will not go down.

Disadvantages :
If the Cache is not available then every time it has to get the data from Database which will impact the performance.

2.Read Through :
----------------
In case of Read Through pattern Cache is placed in between the Application Server and the Database.
Application Server never communicates with the Database.
This kind of approach is more used when there are frequent read operations.
Also the Cache is preloaded with the data to avoid Cache Miss.
In Read Through pattern Caching is implemented by third party Libraries.
Also the Data Modelling between Cache and Database should be similar.
 
3.Write Through :
-----------------
Write through is similar to Read Through.
Read Through will read the data from the Cache where as Write through will write the data through the Cache which inturn writes to the Database.

4.Write Around :
----------------
In case of Write Around Applicatio will read the data from the Cache and writes the data to the Database.
Advantage is reducing one layer to write the data to the Database.
Write Around can be used when there are lot of writes and less no of reads.
 
5.Write Back :
--------------
In case of Write Back All the write requests to an Application are kept in Cache and provides the response saying Write is accepted.
After certain time all the writes in bulk are sent to Database.
Write Back can be used when there are lot of writes.
This pattern can handle DB failure for some time.
Disadvantage is if the Cache goes down all the writes are lost.


############################################################### 18.Database Replication #################################################################

Replication means to have a copy.
Consider the scenario where we have one Database and we want one more Database with the exact copy of the data.
The another Database which has the copy of the data is called Replica.
The Database which is the main source for writes and updates is called Master Data or Primary Database.
The Database which is the main source for reads is called Slave Database or Secondary Database.

Advantages :
------------
1.Having a Replica helps in great dealing with Faults.
For some reasons if the Master Database goes down then in that case having a Replica helps.
First we will not loose the data.
One of the Replica will act as a Master Database.

2.Having Replica helps in reducing the Network latencies.
Placing the Replica's in different Geographic Locations will help in reducing the Network Traffic to access the data.
Based on Geographic Location Data wil be fetched from nearest Data Center.

3.If we are having Replica's it is very common factor that the Master Database will be used for Writes and Updates.
Replica Databases will be used for Read Operations.

4.Consider the scenario of ECommerce Application where we have less no of writes/updates for the Products.
Read operations will be more for the Product details.
Having Replica's allows us to increase the Application performance and increase the system scalability.

Replication Lag :
-----------------
There are lot of complexities while handling the Replications while updating the data,making the data availabe to all the Replica's.
Replication Lag is the time it takes for the value to be copied from Master Database to Replica Database.
Consider the scenario of the request where we have write and read are at the same time.
In this scenario i.e. Replication Lag is higher then we cannot get the updated data from Replica Database and provides the inconsistent data.
Design the System in such a way that we always get the consistent data in terms of writes and reads.
The way these problems are solved called as Consistency Models or Consistency Algorithms.
One of the Consistent Model is Read After Write Consistency.
 
Read After Write Consistency (or) Synchronous Replication :
-----------------------------------------------------------
In this case when a new Request comes it will insert the data into Master Database and 
it will also insert into the Replica Database and gets the ackonwledgment from Replica Database.
Once the acknowledgement is OK then Master Database says the Write is completed.
In this scenario Replication Lag is Zero because Master Database is taking care of updating the Data in all the Replica's and not declaring the write as complete.
This is also known as Synchronous Replication.

Advantage :
-----------
1.Relication Lag is zero.
2.Data is always consistent in the System.

Disadvantages :
---------------
The performance might take a hit because every Write operation has to wait till all the Replica Database has to be updated and acknowledged.
















############################################################### 21.Database Sharding ####################################################################

Consider the scenario of building a User Management System which stores lot of User data.
When the number of User increease we will increease the storage of Database.
What if the Users increases 10 times.
Here the Database Server cannot fit those many Users.
The solution is to partition the data i.e. to store data into multiple databases which is nothing but Database Sharding.

If we have lot of data and increasing the storage does not suffice we end storing the data into multiple machines.
This is also called as Data Partitioning.

Consider the scenario where we have 1 million users and the storage capacity is 256 GB.
Now the Users increased to 2 Million and again we need to increase the storage capacity from 256 GB to 512 GB.
There is some limit in storage capacity and beyond that we cannot increase the storage capacity.

The solution is to divide the data into multiple databases.
If we have 4 Million users then we split the data into 4 different Databases with the capacity of each Database as 1 TB.
Here we are partitioning the data and all of them combine will represent the complete data set.

There are two ways to partition the date.
One way is to store all the columns in different shards.
It is a kind of dividing some of the columns and storing it into different databases.
This is konwn as Vertical Partitioning.

Logical Shard vs Physical Shard :
---------------------------------
Another way is to divide the rows which is nothing but Horizontal Partitioning or Sharding.
Horizontal Sharding is the widely used technique to store large data,highly available and scalable databases.

Shard is nothing but partition of data or subset of the whole data.
The subset could be logical or physical.

Consider the scenario where we have 4 million Users where each database has storage capacity of 1 TB.

Database 1 : 0 to 1 Million  (Partition 1)
Database 1 : 1 to 2 Million  (Partition 2)
Database 2 : 2 to 3 Million  (Partition 3)
Database 2 : 3 to 4 Million  (Partition 4)

Partition1 and Partition2 are logical Shards and sits on Database1 which is nothing but physical Shard.
Finally the whole Database is divided into 2 Physical Databases(Physicl Shards) with 4 Logical Shards.

Advantages :
------------
1.We are not able to store data in Single Database and which runs out of storage.
This will be avoided by partitioning the data.
2.If we want to write teh search the query which has to search in all the records in the database.
The query performance will be slower.
If we partition the data, then the query has to search in one of the database which will improve the performance of the Query.
3.We can place the Physical Shards into different Geographical Locations.
4.If we have single Database then it is a single point of failure.
If we have multiple physical instances even though one the instance is failed,Still we can able to serve the data with the remaining Physical instances.

Sharding strategies :
---------------------
If we have to partition the data based on userid,city or zone etc.
Ther are two types of Sharding strategies.

1.Algorithmic Sharding
2.Dynamic Sharding

Consider the scenario where the Database is being used by our Application for reads and writes.
If the App knows which Partition it has to use for read and write then it is Algorithmic Sharding.
Algorithmic Sharding is static Sharding where we cannot increase or decrease the Shards.

There will be a separate service which tells where the Query has to be routed which is called Dynamic Sharding.
This service will be consumed by the Application.
By using Dynamic Sharding we can increase the decrease the Shards.

Drawbacks of Sharding :
-----------------------
1.The way we are distributing the data is very critical.
If we don't do it correctly the whole data will not be evenly distributed in these partitions.
Some of the partitions are over loaded and some of the partitions will have less data.
2.Once we partition the data and If we want to come back to non sharded architecture then it is very difficult.
3.If there is no other way then only prefer the sharding.

################################################################### 25.Hashing  ###################################################################

Hashing is nothing but a way of assigning a unique value for an Object after applying algorith/function on its properties.
Hashing will return a hashcode in the form of Integer.

Consider the scenario of List of Strings with some names.
For each name if we apply hash function it will generate a unique number.
If we want to search for the particular name then we have to iterate over the List of names.
If we implement the hash function for that then we directly go that index location based on hash value.
Here we are acessing the value using index.

Range of hash values will be from 0 to 2 power 32.
We cannot keep that much array to store these names.
Even if we store 1000 names in an Array then also Array is too large.
If we are going to map the hash values with the index of an Array then also we end up with a large Array which will consume lot of space.


The solution is after calculating the hash we will take the modulous.
Consider the scenario of storing 4 names we wll take twice or thrice the size of an Array.
Then we will take the mod on the hash values.

Ankit  12
Neha   42
Ankur  34
Ruchi  67

Considering the Array size is 8.

0
1
2  Neha Ankur
3  Ruchi
4  Ankit
5
6
7

Here after applying mod the hashcode of Neha and Ankur are the same.
In this case we are storing the data in the form of List in that particular index.
This approach allows us to store key value pairs and acessing key value pairs very efficiently.
The scenario where multiple values are stored in the same index is nothing but collision.

Hashing Use cases :
-------------------
Saving the data into key value pairs will help in Caching.
For example Redis or MemCache which are key value storage.
We can store lot of key value pairs in one single machine and if the size the key value pair increases then we can distribute it across multiple machines.

Consider the scenario of storing the names in 4 different Servers.

Ankit  16  S0
Neha   25  S1
Ankur  30  S2
Ruchi  23  S3

This is how data will be distributed across servers.
If we want to search for Ankit then hash the Ankit by applying mod 4 (Server count) it will give where Ankit has stored.

Disadvantages :
---------------
When the load increases or decreases we have to increase or decrease the Servers.
Consider the scenario we have reduced 1 Server.
In this case the data distributed across the servers will be redistributed.

Ankit  16  S1
Neha   25  S1
Ankur  30  S0
Ruchi  23  S2

Basic condition of scalable system is that it can scale up or scale down the System as the requirement increase or decrease.
We need to handle the scenario where If the no of Servers increse or decrease we have to minimize the data movement.
This can be done by Consistent Hashing.






































