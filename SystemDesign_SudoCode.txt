1.Introduction											[22-JUN-2024]  ()
2.Components of System Design                           [22-JUN-2024]  ()
3.Client Server Architecture                            [22-JUN-2024]  ()
4.Proxies                                               [22-JUN-2024]  ()
5.Data and Data flow                                    [22-JUN-2024]  ()
6.Database Types                                        [22-JUN-2024]  ()
7.Anatomy of Application Services                       [22-JUN-2024]  ()
8.API                                                   [22-JUN-2024]  ()
9.Caching Patterns                                      [22-JUN-2024]  (InProgress)
10.Rest API                                             [22-JUN-2024]  ()
11.Message Queues                                       [22-JUN-2024]  ()
12.Publisher Subscriber                                 [22-JUN-2024]  ()
13.Publisher Subscriber Use Cases                       [22-JUN-2024]  ()
14.Performance Metrics                                  [22-JUN-2024]  ()
15.Performance Metrics of Components                    [22-JUN-2024]  ()
16.Fault and Failure in Distributed System              [22-JUN-2024]  ()
17.Scaling                                              [22-JUN-2024]  ()
18.Database Replication                                 [22-JUN-2024]  (InProgress)
19.CAP                                                  [22-JUN-2024]  ()
20.CAP Theorem                                          [22-JUN-2024]  ()
21.Database Sharding                                    [23-JUN-2024]  (Done)
22.Key based Sharding                                   [22-JUN-2024]  ()
23.Range based Sharding                                 [22-JUN-2024]  ()
24.Directory based Sharding                             [22-JUN-2024]  ()
25.Hashing                                              [24-JUN-2024]  (Done)
26.Consistent Hashing                                   [22-JUN-2024]  ()
27.Capacity Estimation                                  [22-JUN-2024]  ()

############################################################### 1.Introduction ##########################################################################



############################################################### 9.Caching Patterns ######################################################################

https://dzone.com/articles/introducing-amp-assimilating-caching-quick-read-fo


############################################################### 18.Database Replication #################################################################

Replication means to have a copy.
Consider the scenario where we have one Database and we want one more Database with the exact copy of the data.
The another Database which has the copy of the data is called Replica.
The Database which is the main source for writes and updates is called Master Data or Primary Database.
The Database which is the main source for reads is called Slave Database or Secondary Database.

Advantages :
------------
1.Having a Replica helps in great dealing with Faults.
For some reasons if the Master Database goes down then in that case having a Replica helps.
First we will not loose the data.
One of the Replica will act as a Master Database.

2.Having Replica helps in reducing the Network latencies.
Placing the Replica's in different Geographic Locations will help in reducing the Network Traffic to access the data.
Based on Geographic Location Data wil be fetched from nearest Data Center.

3.If we are having Replica's it is very common factor that the Master Database will be used for Writes and Updates.
Replica Databases will be used for Read Operations.

4.Consider the scenario of ECommerce Application where we have less no of writes/updates for the Products.
Read operations will be more for the Product details.
Having Replica's allows us to increase the Application performance and increase the system scalability.

Replication Lag :
-----------------
There are lot of complexities while handling the Replications while updating the data,making the data availabe to all the Replica's.
Replication Lag is the time it takes for the value to be copied from Master Database to Replica Database.
Consider the scenario of the request where we have write and read are at the same time.
In this scenario i.e. Replication Lag is higher then we cannot get the updated data from Replica Database and provides the inconsistent data.
Design the System in such a way that we always get the consistent data in terms of writes and reads.
The way these problems are solved called as Consistency Models or Consistency Algorithms.
One of the Consistent Model is Read After Write Consistency.
 
Read After Write Consistency (or) Synchronous Replication :
-----------------------------------------------------------
In this case when a new Request comes it will insert the data into Master Database and 
it will also insert into the Replica Database and gets the ackonwledgment from Replica Database.
Once the acknowledgement is OK then Master Database says the Write is completed.
In this scenario Replication Lag is Zero because Master Database is taking care of updating the Data in all the Replica's and not declaring the write as complete.
This is also known as Synchronous Replication.

Advantage :
-----------
1.Relication Lag is zero.
2.Data is always consistent in the System.

Disadvantages :
---------------
The performance might take a hit because every Write operation has to wait till all the Replica Database has to be updated and acknowledged.
















############################################################### 21.Database Sharding ####################################################################

Consider the scenario of building a User Management System which stores lot of User data.
When the number of User increease we will increease the storage of Database.
What if the Users increases 10 times.
Here the Database Server cannot fit those many Users.
The solution is to partition the data i.e. to store data into multiple databases which is nothing but Database Sharding.

If we have lot of data and increasing the storage does not suffice we end storing the data into multiple machines.
This is also called as Data Partitioning.

Consider the scenario where we have 1 million users and the storage capacity is 256 GB.
Now the Users increased to 2 Million and again we need to increase the storage capacity from 256 GB to 512 GB.
There is some limit in storage capacity and beyond that we cannot increase the storage capacity.

The solution is to divide the data into multiple databases.
If we have 4 Million users then we split the data into 4 different Databases with the capacity of each Database as 1 TB.
Here we are partitioning the data and all of them combine will represent the complete data set.

There are two ways to partition the date.
One way is to store all the columns in different shards.
It is a kind of dividing some of the columns and storing it into different databases.
This is konwn as Vertical Partitioning.

Logical Shard vs Physical Shard :
---------------------------------
Another way is to divide the rows which is nothing but Horizontal Partitioning or Sharding.
Horizontal Sharding is the widely used technique to store large data,highly available and scalable databases.

Shard is nothing but partition of data or subset of the whole data.
The subset could be logical or physical.

Consider the scenario where we have 4 million Users where each database has storage capacity of 1 TB.

Database 1 : 0 to 1 Million  (Partition 1)
Database 1 : 1 to 2 Million  (Partition 2)
Database 2 : 2 to 3 Million  (Partition 3)
Database 2 : 3 to 4 Million  (Partition 4)

Partition1 and Partition2 are logical Shards and sits on Database1 which is nothing but physical Shard.
Finally the whole Database is divided into 2 Physical Databases(Physicl Shards) with 4 Logical Shards.

Advantages :
------------
1.We are not able to store data in Single Database and which runs out of storage.
This will be avoided by partitioning the data.
2.If we want to write teh search the query which has to search in all the records in the database.
The query performance will be slower.
If we partition the data, then the query has to search in one of the database which will improve the performance of the Query.
3.We can place the Physical Shards into different Geographical Locations.
4.If we have single Database then it is a single point of failure.
If we have multiple physical instances even though one the instance is failed,Still we can able to serve the data with the remaining Physical instances.

Sharding strategies :
---------------------
If we have to partition the data based on userid,city or zone etc.
Ther are two types of Sharding strategies.

1.Algorithmic Sharding
2.Dynamic Sharding

Consider the scenario where the Database is being used by our Application for reads and writes.
If the App knows which Partition it has to use for read and write then it is Algorithmic Sharding.
Algorithmic Sharding is static Sharding where we cannot increase or decrease the Shards.

There will be a separate service which tells where the Query has to be routed which is called Dynamic Sharding.
This service will be consumed by the Application.
By using Dynamic Sharding we can increase the decrease the Shards.

Drawbacks of Sharding :
-----------------------
1.The way we are distributing the data is very critical.
If we don't do it correctly the whole data will not be evenly distributed in these partitions.
Some of the partitions are over loaded and some of the partitions will have less data.
2.Once we partition the data and If we want to come back to non sharded architecture then it is very difficult.
3.If there is no other way then only prefer the sharding.

################################################################### 25.Hashing  ###################################################################

Hashing is nothing but a way of assigning a unique value for an Object after applying algorith/function on its properties.
Hashing will return a hashcode in the form of Integer.

Consider the scenario of List of Strings with some names.
For each name if we apply hash function it will generate a unique number.
If we want to search for the particular name then we have to iterate over the List of names.
If we implement the hash function for that then we directly go that index location based on hash value.
Here we are acessing the value using index.

Range of hash values will be from 0 to 2 power 32.
We cannot keep that much array to store these names.
Even if we store 1000 names in an Array then also Array is too large.
If we are going to map the hash values with the index of an Array then also we end up with a large Array which will consume lot of space.


The solution is after calculating the hash we will take the modulous.
Consider the scenario of storing 4 names we wll take twice or thrice the size of an Array.
Then we will take the mod on the hash values.

Ankit  12
Neha   42
Ankur  34
Ruchi  67

Considering the Array size is 8.

0
1
2  Neha Ankur
3  Ruchi
4  Ankit
5
6
7

Here after applying mod the hashcode of Neha and Ankur are the same.
In this case we are storing the data in the form of List in that particular index.
This approach allows us to store key value pairs and acessing key value pairs very efficiently.
The scenario where multiple values are stored in the same index is nothing but collision.

Hashing Use cases :
-------------------
Saving the data into key value pairs will help in Caching.
For example Redis or MemCache which are key value storage.
We can store lot of key value pairs in one single machine and if the size the key value pair increases then we can distribute it across multiple machines.

Consider the scenario of storing the names in 4 different Servers.

Ankit  16  S0
Neha   25  S1
Ankur  30  S2
Ruchi  23  S3

This is how data will be distributed across servers.
If we want to search for Ankit then hash the Ankit by applying mod 4 (Server count) it will give where Ankit has stored.

Disadvantages :
---------------
When the load increases or decreases we have to increase or decrease the Servers.
Consider the scenario we have reduced 1 Server.
In this case the data distributed across the servers will be redistributed.

Ankit  16  S1
Neha   25  S1
Ankur  30  S0
Ruchi  23  S2

Basic condition of scalable system is that it can scale up or scale down the System as the requirement increase or decrease.
We need to handle the scenario where If the no of Servers increse or decrease we have to minimize the data movement.
This can be done by Consistent Hashing.






































