1.Volatile vs Atomic Integer						[23-NOV-2025]  (Done)
2.ThreadLocal										[05-JAN-2020]  ()
3.Concurrency vs Parallelism						[08-JAN-2020]  (Done)
4.Java Memory Model									[15-JAN-2020]  (Happens Before Pending)
5.ExecutorService Introduction						[20-JAN-2020]  (Done)
6.ExecutorService Pool Types						[21-JAN-2020]  (Single ThreadExecutor Pending)
7.ExecutorService LIfe Cycle						[21-JAN-2020]  (Done)
8.ExecutorService Callable/Future					[27-JAN-2020]  (Done)

9.Java Asynchronous Programming						[28-JAN-2020]  (Done)

10.Lock Condition Class								[05-JAN-2020]  ()
11.Completable Future								[28-JAN-2020]  (Done)

12.Coroutines										[05-JAN-2020]  ()
13.Reentrant Lock									[06-JAN-2025]  (Done)
14.ReadWrite Lock									[06-JAN-2025]  (Done)
15.Java Interrupts									[05-JAN-2020]  ()
16.Semaphore in Java Concurrency					[05-JAN-2020]  ()
17.Concurrency Interview Questions                  [05-JAN-2020]  ()
18.Concurrency Interview Questions                  [05-JAN-2020]  ()
19.Concurrency Interview Questions                  [05-JAN-2020]  ()
20.Fork JoinPool									[27-JAN-2020]  (Done)
21.Phaser vs CountDownLatch vs Cyclic Barrier		[05-JAN-2020]  ()

22.Synchronous Queue								[05-JAN-2020]  ()
23.Adder and Accumulator Classes					[05-JAN-2020]  ()
24.Guava Library									[05-JAN-2020]  ()

25.Detect and Resolve DeadLocks in Java				[05-JAN-2020]  ()
26.Race Condition vs Data Races                     [05-JAN-2020]  ()
27.Singleton and Double Checked Locking             [05-JAN-2020]  ()

28.Spin Locks                                       [05-JAN-2020]  ()
29.Exchanger Class									[05-JAN-2020]  ()


############################################ 1.Volatile vs Atomic Integer ############################################################

Consider the scenario where we have one flag i.e.

	boolean isDevice=true;
	
Also we have two threads i.e. Thread1 and Thread2 running in paralell.
In Thread2 we have while condition which accepts the flag. If the flag is true it will execute in infinite loop.
To ensure while loop not to execute in infinite loop, we make the flag as false in Thread1.
We expect that changes done in Thread1 will impact in Thread2, but as per Java specification it won't impact the changes.
To understand the reason how the CPU caches are structured.

Consider the CPU with 2 cores where Thread1 sits on core1 and Thraed2 sits on core2.
Each of these cores has local cache and there will be a shared cache.

Initially the value of the flag is stored in their Local cache.
When Thread1 updates the falg to false, Thread2 still not get the updated flag.
This is the reason Thread2 will run in infinite loop.
Here the update is not visibile to Thread2 and is the visibility problem.

One solution to resolve the problem is by adding the volatile keyword.
When we use volatile, every read/write to go through Shared cache, not per-thread Local cache and invalidates the other Core Cache.
Now Thread2 has to read the value from the Shared Cache and will get the updated value.
Volatile is genarally used to solve the visibility problem.

✅ Volatile – Visibility flags
Best when:
Only one thread writes
Many threads read
No compound updates

	volatile boolean running = true;

Atomic operation :
------------------
Consider the same scenario using Integer value.

	int value = 1;
	
	// Thread 1
	value++;   // value = value + 1

	// Thread 2
	value++;   // value = value + 1
	
Here value++ is not Atomic and is a compound operation consisting of three steps.
1. READ  value from memory
2. ADD   1 to the local copy
3. WRITE updated value back to memory

	Initial value = 1

	T1 reads 1
	T2 reads 1

	T1 adds -> 2
	T2 adds -> 2

	T1 writes 2
	T2 writes 2   (overwrites T1's result)

✅ Final value = 2
❌ Correct result should be 3
This is called a Race Condition

	volatile int value = 1;
	
Both threads can see each other’s updates if volatile is used.
The error still happens because the update itself is not Atomic.
The outcome is still incorrect.

Thread1 tries to increment the value and also Thread2 tries to increment the value i.e. value++ i.e. value=value+1;
When two threads are running paralelly, we will not get the changes done in Thread1 available to Thread2.
Even if we apply the volatile also still it will not work.
Here the problem is not a visibility problem and is synchronization problem.
Here the operation is read the value,add 1 to it and write the value.
Here the main reason is we have compound operation and we don't have one atomic operaton which will read and write the updated value.
To resolve this problem we can also use synchronized keyword i.e. only one Thread is allowed to increment the value.

The other solution to resolve this problem is using AtomicInteger.
AtomicInteger uses hardware CAS operations internally so that the whole read-modify-write cycle is executed atomically without locks.

Volatile is used for flags.
AtomicInteger is used for Counters.
AtomicReference is used for Cache.

CAS(Compare-And-Swap) :
-----------------------
AtomicInteger solves this by using CAS(Compare-And-Swap)
AtomicInteger uses a hardware CPU instruction.
This happens 100% atomically in hardware and not in Java code.

	CAS(expectedValue, newValue) i.e. Only write the new value if the current value equals expectedValue.
	
AtomicInteger counter = new AtomicInteger(1);
counter.incrementAndGet();

Internally JVM performs:

	do {
		current = value;      // Read
		newValue = current+1; // Modify
	} while (!CAS(current, newValue)); // try once, retry if failed
	
value : Shared AtomicInteger i.e. initial State
current : Local variable in the thread
newValue : Desired updated value

This is how AtomicInteger.incrementAndGet() and similar methods are safe in a multithreaded environment without using synchronized.

############################################ 2.ThreadLocal ###########################################################################


############################################ 3.Concurrency vs Parallelism ############################################################

Parallelism :
-------------
Let us consider an example.

	public static void main(String[] args) 
	{
		
		new Thread(new Runnable() {
			public void run() {				--> Task One
				processTax();
			}
		}) {
		}.start();
		
		
		new Thread(new Runnable() {
			public void run() {				--> Task Two
				processTax();
			}
		}) {
		}.start();

		hevayCalculations();				--> Task Three
	}

Here Task One and Task Two runs on two different threads and Task three runs on the Main thread.
If we run this program on a Quad Core Processor i.e. there are 4 cores in a processor, these tasks will run in parallel.
In OS we have a Component called Scheduler which is responsible to schedule these threads on to the CPU Processor.

Task One runs on Core 1
Task Two runs on Core 2
Task Three runs on Core 3
Task Four runs on Core 4

Here all these tasks will run in separate cores in Parallel.
Parallelism is about doing lot of things at once so that we can speed up performance.

We can also change the above code by using ExecutorService Fixed Thread Pool.

		ExecutorService executorService=Executors.newFixedThreadPool(4);
		executorService.submit(()-> processTax());
		executorService.submit(()-> processTax());
		
In Java to enable Parallelism we use raw threads(implementing Runnable or Callable Interface) or we can use the concept of Thread Pool.
Even in Thread Pool it can be 

1.ExecutorService
2.ForkJoin Pool
3.Custom Thread Pools (Used by Web Servers)

Here we need to ensure that the CPU has more than one Core so that we can run the threads in parallel.

Concurrency :
-------------
Consider the scenario of booking a ticket.

		new Thread(() -> {
			if(ticketsAvailale>0)
			{
				bookTicket();				-->  Task accessing Shared Variable
				ticketsAvailale--;
			}
			
		}).start();
		
		new Thread(() -> {
			if(ticketsAvailale>0)
			{
				bookTicket();			 	-->  Task accessing Shared Variable
				ticketsAvailale--;
			}
			
		}).start();
		
		Thread.sleep(200);
		
Here each tasks shares the common resource i.e. ticketsAvailale variable.
After starting two Threads put the Main Thread in Sleep for two seconds.
Let us assume that we have a Single Core Processor in CPU and OS Scheduler is responsible for assigning these Threads to the processor.
Since CPU has single Core Processor,Scheduler has to do time sharing between the threads.
Here the Scheduler will Schedule the Thread1 for few Milli seconds and pick Thread1 out and keep Thread2 for few Mill seconds.
This concept is called interleaving of Threads.
How much time Scheduler assigns to the Thread is non-deterministic and depends on many factors.
Here it might possible that Thread1 accomplishes all the tasks within time.
In worst scenario if the Thread has not accomplished the Task then there is mismatch in processing.
This is also the case with MultiCore processor.

To resolve the issue we use the concept called Lock.
Thread One will acquire the Lock,Completes the Task and updates the shared variable and release the task.
Here Multiple Threads are coordinating between each other using a single lock.
This scenario is called Concurrency.

Concurrency is about dealing lot of things once.
Concurrency is applied when we have a shared resource that has to be accessed or updated or Multiple Threads needs to coordinate.

In Java we have lot of tools to deal with Concurrency.

1.Locks /Synchronized
2.Atomic Classes
3.Concurrent Data Structures (ConcurrentHashMap.Blocking Queue)
4.Completable Future
5.CountDownLatch / Phaser / CyclicBinder / Semaphore etc.

Concurrency  + Parallelism  :
-----------------------------
Split the sequential flow into independent components. i.e. Use Threads / Thread Pool to parallelize these Components.
Whenever a shared resource has to be updated across the Components or Multiple Threads needs to be coordinate use Concurrency tools to manage.


############################################ 4.Java Memory Model ############################################################

Java Memory Model is a specification which guarantees the visibility of fields when we have re-ordering of instructions.
Java Memory Model enforces that all the JVM's has to implement these set of rules so that program runs in the same manner. 

1.Order of Execution
2.Field Visibility
3.Happens Before

1.Out of Order Execution :
--------------------------
When we write a program and in general it is a series of statements.
We expect that it will run in the same order as we have written.
But is completely possible that the JVM or CPU will change the Order of instructions to drive some performance out of it.
This is done in such a way that the program semantics remains same and output will not change.
Within the same thread, JMM guarantees that final result must remain the same.

Consider the Scenario 

a=3;
b=1;
a=+1;

Actual conversion look Like 

Load a
Set to 3
Store a

Load b
Set to 1
Store b

Load a
Set to 4
Store a

Here we are Loading a twice and 
one Performance improvement could be incrementing the value would be before setting the Value.

Load a
set to 3
set to 4
Store a

Load b
Set to 1
Store b

If multiple threads access shared variables, reorderings may produce incorrect results unless JMM rules enforce correct ordering like happens-before.
 
2.Field Visibility (The Core Multithreading Problem) :
------------------------------------------------------
Due to multi-core CPU architecture, each thread may see different values of the same variable.

A.If we have a Quad Core Processor generally that have 4 Cores and each core has set of registers.
B.Registers are nothing but a small memory area within the core,Stores the value in the register and it is faster to retrieve the value from the register.
C.we have L1 Cache for each Core i.e. Memory area set a side for each Core.
D.We have L2 Cache which could be shared between Shared L1 Cache.
E.We have L3 Cache which could be Shared between Shared L2 Cache.
F.We have RAM which could be Shared across Cores.


			Core1      		Core2  		Core3        Core4
		   registers	  registers	   registers    registers
		    L1 Cache	   L1 Cache     L1 Cache     L1 Cache
			          L2 Cache                   L2 Cache
								   L3 Cache
								      RAM
			
Consider the Example :

			int x=0;
	
			public void writerThread()
			{
				x=1;
			}
			
			public void readerThread()
			{
				int reader=x;
			}			

Here initially the value is stored in the Shared Cache.
Consider thread T1 calls the WriterThread() and thread T2 calls the ReaderThread().
Thread T1 sits into the Core 1 and Thread T2 Sits into the Core 2.
When the Thread T1 executes the writerThread(), value is incremented and updated in the Local Cache of Thread T1.
Similarly when the Thread T2 executes the readerThread(),it won't get the updated value because it is not updated in the Shared Cache.
It is completely possible that x=1 is never pushed into the Shared Cache.
This issue is called Field Visibility issue.

We can resolve the issue by using volatile key word.
If the variable is volatile,then JVM ensures that changes in the Local Cache will be flushed to the Shared Cache so that the remaining Threads will get the updated value.

3.Happens Before :
------------------


############################################ 5.ExecutorService ########################################################################################

Introduction :
--------------
In Java it is easy to run a piece of code or method asynchronously by using Threads.
Consider the main() which runs on Main Thread and we can run the task on the separate Thread.
To create a Task we need to implement Runnable interface and override its run() and keep the method inside run() that we want to execute asynchronously.

				public static void main(String[] args) 
				{
					Thread thread=new Thread(new Task());
					thread.start();
					System.out.println(Thread.currentThread().getName());
				}
				
				public class Task implements Runnable
				{
					public void run()
					{
						System.out.println(Thread.currentThread().getName());
					}
				}

Here Main Thread will perform the operations from top to bottom and at certain point of time it will call start() and it will create a Thread with name thread-0.
thread-0 will do the operation on its own and once the operation is done thread will be killed automatically.
Here Main Thread continues its execution.

In the above example we are creating single task.
Consider the Scenario where we want to execute 10 tasks and in this case we need to create Thread using for-loop.

			for(int i=0;i<=10;i++)
			{
				Thread thread10=new Thread(new Task());
				thread10.start();
				System.out.println(Thread.currentThread().getName());
			}

Here each time it will create a new Thread with the new instance of the Task and that will be executed asynchronously.
Here the flow starts from Main Thread and after certain point of time for-loop is getting executed.
Here each time new Thread is being created and will be executed asynchronously.
once the task is executed successfully thread will be killed automatically.

Problem with this approach is, In Java one Thread is equal to one Operating System Thread.
If we run the for-loop 1000 for times, it will create 1000 Threads and creating a Thread is an extensive operation.

If we want fixed number of Threads to be created as Thread Pool and then submit the Task to the Thread Pool, Then we use ExecutorService.

			ExecutorService executorService=Executors.newFixedThreadPool(10);
			for(int i=0;i<=1000;i++)
			{
				executorService.execute(new Task());
			}

Here instead of creating new Threads every time,Here we are creating threads upfront and submitting the task to the threads which are created already.
Thread Pool internally uses Queue and keeps storing all the tasks which are submitted.
Threads in the Thread Pool will perform two steps i.e. fetch the next Task from the Queue and execute it.
Since all the Threads are executing simultaneously to get the Task from the Queue,Queue has to handle Concurrent modifications and should be Thread Safe.
To resolve this issue Queue in the Thread Pool is implemented using Blocking Queue.

Producers: The Thread calling execute() i.e. usually our Main thread, web request threads etc.
Consumers: The 10 worker Threads inside the pool.

We need a solution where
1.Store the tasks when all 10 threads are busy.
2.Give tasks one-by-one to worker threads when they are free.
3.Ensure all of this is thread-safe when many threads submit tasks and many threads execute tasks.

This is where BlockingQueue comes in.
A BlockingQueue<Runnable> is a thread-safe queue with two special properties.

put(task) :
If the queue is full, Producer waits (blocks) until space is available.

take() :
If the queue is empty, Consumer waits (blocks) until a new task is available.

Here only one thread manipulates the internal queue structure at a time due to lock.
Others wait efficiently using OS-level wait (park/unpark).OS deschedules it and CPU is free for other tasks.

	ExecutorService pool = Executors.newFixedThreadPool(5);
	public void handleRequest(Request req) {
		pool.execute(() -> generateReport(req));
	}

Scenario :
----------
At 10:00:00, 20 users hit /generate-report at the same time.
Our Application thread receiving these requests does.
20 calls to pool.execute(...) i.e. 20 tasks in BlockingQueue.
Internally 5 worker threads immediately take 5 tasks and start generateReport.
Remaining 15 tasks stay in the Queue.
Whenever a worker finishes a report,It calls take() again.
Gets the next pending task from the queue.

Scenario :
----------
If there are no tasks in the Queue, Then Worker Thred will move to WAITING state.
This Thread doesn't need CPU and will be suspended.
When new Task arrives, Queue signal the Thread, WAITING Thread wakes and moves to Runnable state, Picks the Tasks from the Queue and executes it.

Ideal Pool Size :
-----------------
If we have created more number of Threads i.e. 1000 , these Threads will be executed based on Time Slicing.
Solution depends on the type of task we want to execute.

If the task is CPU intensive operation i.e. If the task is taking time to execute then the ideal Pool Size will be the same as CPU core size.
Here all the Threads will be executed in the CPU cores independently and none of the Threads are waiting for anything.
Consider the Scenario where other applications are running on the CPU, mostly we wont get all the CPU Cores.

			int processors=Runtime.getRuntime().availableProcessors();
			ExecutorService execService=Executors.newFixedThreadPool(processors);
			for(int i=0;i<=1000;i++)
			{
				executorService.execute(new CPUProcessor());
			}
			
If the task is an I/O Operation i.e. It may be a call to the database or call to an external service then we need to create a Thread Pool with maximum Size.
Because If any of the Thread is time consuming and after certain time No threads will be available to pick up the task from the Queue.
Here Pool Size will be the same as the number of tasks we are submitting.
Too many Threads will increase the Memory Consumption too.
 
			ExecutorService execService=Executors.newFixedThreadPool(1000);
			for(int i=0;i<=1000;i++)
			{
				executorService.execute(new CPUProcessor());
			}
			
Fixed Thread Pool :
-------------------
A fixed number of worker threads are created upfront when the thread pool is initialized. 
Whenever we submit tasks, all the tasks are added to an internal work queue. 
This queue is a thread-safe BlockingQueue that ensures safe task submission and retrieval in a concurrent environment.
The worker threads repeatedly take tasks from the queue (blocking if the queue is empty), execute them, and then return to take the next task. 
Multiple worker threads run in parallel — not one after another — until all submitted tasks are completed.
   
			ExecutorService execService=Executors.newFixedThreadPool(10); // ThreadPool Initialization
			for(int i=0;i<=1000;i++)
			{
				executorService.execute(new CPUProcessor());
			}
			
Cached Thread Pool :
--------------------
A Cached thread pool does not have a fixed number of threads and does not use a traditional queue to store the submitted tasks. 
Instead, it uses a SynchronousQueue, which does not store tasks at all and it only hands them off directly from the producer thread to a worker thread.

When a task is submitted, Cached thread pool tries to immediately transfer the task to an existing idle thread. 
If an idle thread is available, it picks up the task immediately.  
If no idle thread exists, the pool creates a new thread to execute the task.

Threads that remain idle for 60 seconds are automatically terminated and removed from the pool, allowing the pool to shrink during periods of low load.

			ExecutorService cachedPool=Executors.newCachedThreadPool();
			for(int i=0;i<=1000;i++)
			{
				cachedPool.execute(new CPUProcessor());
			}
 
Scheduled Thread Pool :
-----------------------
ScheduledThreadPool can be used for the schedule the tasks to be executed after a certain delay.

		ExecutorService scheduledPool=Executors.newscheduledThreadPool();
		scheduledPool.schedule();
		
If we want to perform checks like Security Checks,Logging Checks for every 10 Seconds we use scheduleAtFixedRate().
Here tasks Keep on executing for every 10 Seconds.

		ExecutorService scheduledPool=Executors.newscheduledThreadPool();
		scheduledPool.scheduleAtFixedRate(10);

Here it will store all the tasks in the Queue and this Queue is a Delayed Queue.In Delayed Queue tasks might not be in sequence.
The Tasks will be distributed based on when the task needs to be executed i.e. Time interval.

If we want to run the task for every 10 Seconds after the completion of previous task with delay in seconds we use scheduledPool.scheduleAtFixedDelay(10,15);
Consider the scenario where the task itself takes 15 Seconds and we cannot say  that run the task for every 10 Seconds.
So here in case of scheduleAtFixedDelay() it will complete the task that takes 15 Seconds and after that it will delay for 10 Seconds and schedule the next instance of the task.

Single ThreadExecutor :
-----------------------


Executor Service Life Cycle Methods :
-------------------------------------

shutDown() :
shutDown initiates() the graceful shutdown. New tasks are rejected, but already submitted tasks which are either running or queued in the Thread Pool are allowed to complete.
Example : Gracefully stop a service or batch job.

		New task submission ❌ rejected immediately
		Running tasks ✅ allowed to finish
		Queued tasks ✅ allowed to execute

isShutDown() :
isShutDown() returns true if shutdown has been initiated.

		✅ true → shutdown process has started
		❌ false → If the pool still accepting tasks

isTerminated() :
Returns true if all the tasks are finished execution, threads have stopped, and the executor is fully terminated.

		✅ true ONLY if:
		shutdown() has been called
		AND
		ALL tasks completed
		AND
		ALL threads stopped


awaitTerminattion(10,milliseconds) :
Blocks the calling thread until the executor terminates or the timeout occurs.
This method does not trigger shutdown() and shutdown() must be called first.

shutDownNow():
Attempts immediate shutdown by interrupting running tasks and clearing the queue.
It will return all those task which are stored inside the Blocking Queue.


ExecutorService Callable/Future :
---------------------------------
We can submit the task by using execute() of ExecutorsService where as we can define the task by using Runnable or Callable Interface.
When we execute the task using executor it won't return any result.
Runnable task does not throw any Checked Exception.

		ExecutorService executorService=Executors.newFixedThreadPool(4);
		executorService.execute(new Task());
		
			class Task implements Runnable 
			{
			@Override
				public void run() {

			}
		}
		
Use Runnable only when only side-effects are needed. For example
Logging
Writing to DB
Sending emails
Updating caches
Triggering workflows
		
To get the result submitted by the task we use Callable Interface.
submit() returns Future Object.
Main Thread submits Callable → Thread Pool returns Future immediately → Worker thread runs call() → Result stored internally inside Future → Main thread retrieves result later.
Once Main Thread calls the get(),then Thread goes into Blocking State if the task is not completed.

		ExecutorService executorService1=Executors.newFixedThreadPool(4);
		Future<String> result = executorService.submit(new CallableTask());
		// Main thread continues executing other logic
		System.out.println("Doing other work");
		// Later
		String value = result.get();  // blocks if result not ready
		
		
		class CallableTask implements Callable<String>
		{
			@Override
			public String call() throws Exception 
			{
				return "Srinivas Sankoji";
			}
		}

One way to resolve the Blocking Operation of get() is to use the TimeOut.
get() provides a overloaded version with Timeout as parameter.
		
		result.get(12,TimeUnit.MILLISECONDS);// Blocks untill Result ready OR Timeout expires4
		
Note :
------
execute() accepts a Runnable task where as submit() accepts a Callable task.


######################################################## 9.Java Asynchronous Programming #######################################################

Most of the Computers today we have More than 1 Core Processor.
Typically a Desktop Computer have 4 Cores and Server may Contain 16 Cores or 32 cores or More.
To take Advantage of these cores we create Multiple Threads in our Application.
We can do that by Using 

new Thread().start()
Thread Pool
Fork join Pool and so on.

Creating too Many Threads in Parallel May cause Some Problems and that is because of How Java Works.
In Java Every Thread we Create is an Operating System Thread or Native Thread or Kernel Thread.
Java itself has Variables For every Thread Like Program Counter,Java Stacks,StackFrames and so on.

For Every Thread,there will be a Corresponding OS Thread and that will consume lot of Memory and 
that Limits the Number of Parallel Threads or Active Threads in JVM(Application that runs on JVM).
We Cannot have thousands of Threads in our Application and that will throw out of Memory Exception and the Program will Shut Down.

Once we have Too Many Threads there are Other Problems that may come out.
Consider we have lot of Threads and in CPU we have 2 Cores.
Every Core will have some Local Cache and Consider Core1 is running the Thread 1.
Local Cache has all the Data which is Required by Thread 1.
If there are Lot of Threads and we need to Schedule some other Threads at Some Point of Time.

If we want to Schedule Thread 3 and for that we Need to Flush the Local Cache which Contains the data related to Thread 1
and Place the Data Related to Thread 3 in the Local Cache.
Here Core 1 will Start executing the Thread 3.

Again If there is Context Switch,Flush the Old Thread Data and add The New Thread Data.
This Scenario is Called Data Locality.

If there are lot of Threads,OS or JVM has to manage the Scheduling of Threads which itself takes lot of Time.
The Issue of Having too Many Threads is even More Heighten when  we are Dealing with I/O Operations.

Consider the Main Thread which is performing I/O Operation.
I/O Operation may be a Call to Another MicroService or Database Operation.

When Main Thread Triggers that Particular I/O Operation,then Main Thread will goes into Blocking State
Until The I/O Operation is Completed and Main Thread cannot do anything  else and goes into Waiting State.
Here CPU Cycles are Wasted.
Once the Data is Returned from the I/O Operation Main thread goes into Runnable State and Performs the Remaining Process.

The Problem of Having a Blocking Thread Means It will not perform any Other Operations while the operation is being Completed.

The Ideal Solution is Non Blocking I/O.
Main Thread Trigger the I/O Operation and Immediately Comeback without Taking the Result.
Once the Data is Ready from I/O Operation,then call the Callback Method and will be called by using another Thread.
Here Main Thread keep on Performing Subsequent Operations.

For Loop with Task Defined using ExecutorService will always goes into Blocking State.
Here For Loop goes into Waiting State.

Creating lot Number of  Threads becomes Expensive and Blocking I/O Operations makes limit to Scale the Application.
Here the Solution is Non Blocking I/O and Instead of Synchronous API we Need Asynchronous API.

Asynchronous API is Nothing But Callback and can be achieved by using Completable Future in Java 8.  
For Non Blocking API Java 7 Introduced the Concept of Java NIO and also called as NIO Package.
Servlet 3.1 Onwards the Web Request Becomes Non Blocking.
Spring Web Flux is a Non Blocking API and comes with Push Operation where as Future is a get Operation.

Advantages of Using Asynchronous Programming :  
Efficient CPU Utilization  
Scalability and High Throughput  
No Issue of Data Locality and Less Context Switches 
Reactive  
Live Values 	
Back Pressure  

Issues in Asynchronous Programming :
Hard to Write and Reason About	
CPU Bound Flows	
Hard to Debug and Stack Trace	
Hard to Write Test Cases	
End to End Asynchronous / Non Blocking  Required	

To Achieve Light Weight Threads and Synchronous Programming there is an Upcoming project Coming Soon i.e Java Fibers or Project Loom.


######################################################## 13.Reentrant Lock #################################################################

Refer in Java_MultiThreading_Medium

######################################################## 14.ReadWrite Lock #################################################################

Refer in Java_MultiThreading_Medium

######################################################## 11.Completable Future #############################################################

Completable Future is used to perform asynchronous operations and trigger its dependent operations which could also be asynchronous.

In Java To Perform Non Blocking Operation is Always Easy i.e Create Runnable and run it in Separate Thread.
Once Runnable calls the run() and after the Execution of run() Thread is Destroyed.
If The Task is Expecting the Return Value,then We Use the Callable Interface.We Typically do this by Using ExecutorService.
Here the Task return the Future Object and when we Call get() on the Future Object then Main Thread goes into Blocking State.
Here the Problem gets More Difficult when we have Multiple Tasks.
Consider the Scenario where we have submitted 4 Tasks to the ExecutorService.
Immediately It will return a Future Value with the Place Holders.
Task 3 and Task 4 are Executed Successfully and Task 1 and Task 2 are Still Waiting for the Response because 
Thread Pool does not Guarantee that order of Execution of Tasks.
If we call future.get() and we don't have value for the Task 1 the Main Thread goes into Waiting State.
Here the Problem is even though many Task were Completed and we are Using For Loop to get the Future Object
and For Loop Starts its Iteration with First Element.
Since we don't have value for Task 1 then Main Thread goes into Waiting State.

Consider the Use case :

Fetching Order
Enriching Order
Payment
Dispatch
Send Email

For Order Flow we Have 5 Different Tasks.

If we Use the ExecutorService then we have to Create 5 Tasks and Using Callable and submit it to the ExecutorService.
Here Each Task is Dependent on Another Task and we use future.get() to get the Result and it goes into Blocking State.
If we have For Loop Over the Orders an to Create On Order2,Order3,Order4 Until the Completion of Order1.
Here Main Thread will not Scale Much Further and It is almost Sequential Operation order1 has to be complete before order2.

There are Dependency on order of Execution.
It is Completely Possible that a Thread can do a Payment Process for One Order while Other Thread Performing Enrichment for Another Order.

We have N Number of Threads all doing processing of One Particular Flow.
Within One Flow Tasks are Dependent on Each Other But One Flow is not Dependent on Another Flow.

If We Convert into an Algorithm i.e for N Number of Orders We want to run the Task Once and once Particular Task is Completed,run its Dependent Task and so on.
This has to be Done Completely Independent for Every Task that we Submit.
Don't Care about Thread Pool is Going to Execute those Tasks Internally and here Main Thread does not goes into Blocking State.
This is Exactly Completable Future was Designed for.
Completable Future Provides a Function called supplyAsync() that accepts a Task and has to be executed Asynchronously.
Once the Task has been Executed Successfully give the output to another Task using thenApply()
There is also an Overloaded Method thenApplyAsysnc().In This Case the Thread which is Performing supplyAsync() 
and other Thread will perform the subsequent Operations.
thenApplyAsysnc() can be used if we define our own Thread Pool.
One Thread does not Belongs to Two Different ExecutorService and in This case we use overloaded method of thanApply().
If we don't use any Executor Service Internally it Uses Fork Join Pool to run all the Tasks.
If any of the Operation is Failed exceptionally will be called as a Catch Block.
For Complex Use cases Completable Future is Not Recommended and Prefer RX Java which is Stable and Much More Readable.


############################################ 20.Fork JoinPool ########################################################################################

Fork JoinPool is Similar to Executor Service but with Two Differences.
ExecutorService Stores the Task in the Blocking Queue and these Tasks are executed by the Thread Pool which we Have defined while defining the Executor Service.
These Thread Takes the task from the Blocking Queue and executes it until all the tasks are Completed in the Blocking Queue.

1.Fork Join Pool Deals with Task and produces its own Task.
Fork Join Pool Optimize the Problem of Task by Producing Sub Tasks for the Task.

Let us Consider a Task and that is Big Task.
Break down the Task into 3 Sub Tasks and each of them Computed or solved Independently.
Here all the Sub Tasks run in Parallel at once considering we have CPU which has More than one Core.
All the Results are aggregated to provide the Final Result.

Consider the Example of Fibonacci Example.
Fibonacci Number is a Sequence where every Number in the Sequence is a Multiple of Last Two Numbers.

2.Per Thread Queuing  
In case of Fork Join Pool Each Thread has its own Queue and this type of Queue is called Double Ended Queue.
Consider a Thread is executing a Particular Task and when we do Fork i.e split the Task into Multiple Sub Tasks and all those Sub Tasks are Not Stored in the 
Common Queue and they are Stored in its own Double Ended Queue Locally.
Now for the Thread which is Executing the Main Task,It is Easier to pick the Task which is Stored inside the Double Ended Queue.

Now Every Thread which is executed in the Fork Join Pool has its own Doubled Ended Queue to Store all the Sub Tasks that it Creates.
We don't have to Worry about getting the Tasks from the External Queue.
It Keeps Picking the Tasks from the Local Queue and keep executing them.
Since we are Touching our own Queue and we don't have to worry about Synchronization.
Since Threads are Always Busy there won't be any switch between context of Threads.

3.Work Stealing
Consider Two Threads Thread1 and Thread2
Thread1 Produces lot More Sub Tasks than Thread2
At Certain Point of Time Thread2 has No Tasks to Execute i.e its own Dqueue is Empty.
In this Case Thread2 Picks the Task form Thread1 using the Other End of the Queue i.e
Thread 1 Picks the Task from Front where as Thread 2 Picks the Task from Back.


To Execute The Fork Join Pool in an Effective Way
1.Avoid Synchronization
2.Do Not Use Any Shared Variables
3.Do Not Perform any Blocking Operations
4.Pure Functions
5.Isolated Functions

Fork Join Pool has the Same Methods that has Future Interface.
Use Cases of Fork Join Pool are 

1.Sorting
2.Matrix Multiplication
3.Best Move Finder for a Game
4.Tree Traversal




###################################################### 21.Phaser vs CountDownLatch vs Cyclic Barrier #########################################################

Refer in Java_MultiThreading_Medium

###################################################################### End ###################################################################################



  

 
















		





