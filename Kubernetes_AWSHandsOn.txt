1.Introduction												[28-MAR-2021]  ()
2.Setup MiniKube                                            [31-MAR-2021]  (Done)
3.Kube API Server                                           [31-MAR-2021]  (Done)
4.ETCD in Kubernetes                                        [01-APR-2021]  (Done)
5.Kubernetes Certified Exam                                 [01-APR-2021]  ()
6.Kube Controller Manager and Scheduler                     [01-APR-2021]  (Done)
7.Pods                                                      [12-APR-2021]  (Done)
8.Pod Networking                                            [12-APR-2021]  (Done)
9.Static Pods                                               [27-APR-2021]  ()
10.Container Probes                                         [27-APR-2021]  ()
11.Replica Sets                                             [27-APR-2021]  (Done)
12.Deployment in Kubernetes                                 [28-APR-2021]  (Done)
13.Kubernetes Services Part 1                               [28-APR-2021]  (Done)
14.Kubernetes Services Part 2                                [04-APR-2021] ()

15.Kubernetes Namespace                                     [05-APR-2021]  ()
16.Manual Scheduling                                        [05-APR-2021]  ()
17.Kubernetes Affinity and Anti Affinity                    [05-APR-2021]  ()

18.Kubernetes Taints and Tolerants                          [06-APR-2021]  ()
19.ConfigMaps                                               [06-APR-2021]  ()
20.Persistent Volumes and Claims                            [06-APR-2021]  ()

21.ETCD Backup                                              [07-APR-2021]  ()
22.ETCD Database Restore                                    [07-APR-2021]  ()
23.Nginx Ingress Controller                                 [07-APR-2021]  ()



*************************************************************  3.Kube API Server  ***********************************************************************************

API Server is the primary componenet that we intercat with in the Kubernetes.
When we run the kubectl command,we are actually intercating with API Serevr.
API Server exposes some API that the user can interact with.we can also intercat with API server using curl Request.
Consider the scenario of creating a Pod and the Steps are 

create the yml file
apply

Here request goes to the API Server and creates the Pod object and updates the ETCD and shows the user that Pod object is created.
The Scheduler which is running in the Kubernetes Cluster continuously monitors the API Server for the New Objects.
Whenever Scheduler finds the new Pod object has created,It looks for the Node where the Pod has to be placed.

Scheduler comes with the information of the available Node and then transfers the information to the API Server.
API Server again updates the ETCD Node with the Node information.
API Server then sends the information to the kubelet which is running on the worker Node.
kubelet gets the Node information from the API Server and communicates with the Container Runtime to create the Pod.
Container Runtime actually creates the Pod and updates the kubelet and then kubelet then updates the API Server that the Pod has created.
API Server then updates the ETCD about the Pod creation.

ETCD is the place where we have complete information about the Cluster.

If we install the Kubernetes as Hardware,download the binary and provision the kube API Server.
If we install the Kubernetes using the kubeadm,kubadm actually runs the API Server as Pod.
When we switch to the console we can actually see how the API Server is running as Pod and we can see all the Configuration details.

To get all the Nodes in the Cluster,we use the command

	kubectl get nodes

To get all the Pods in the Cluster,we use the command

	kubectl get pods

Here no Pods are running as of now.
minikube actually uses the kubeadmin to deploy the Kubernetes cluster.
Here we are not able to see all the Kubernetes Components as Pods because all these Pods are deployed in different Namespaces.
All these system Pods runs in a namespace called kube-system.

	kubectl get pods --namespace kube-system (or) kubectl get pods --n kube-system

If we are deploying Kubernetes as a Hardware,download the binary and run the kube API Server as a Service.
Configuration will be in the ETCD  and can view the Configuration in the ETCD service filename.
To see the configuration of the API Server Pod,we use the command

	kubectl describe pod kube-apiserver-minikube -n kube-system

If we want to see the response in Json format

	kubectl get pods -o json

*************************************************************  4.ETCD in Kubernetes  ******************************************************************************

ETCD is the single source of truth for all the components of the Kubernetes System.
ETCD actually stores everything that is happening in the Kubernetes Cluster i.e Pod Creation,Adding Node,Any Deployment etc.
When we do migration in Kubernetes,ETCD is the most importnat Component and If we loose the ETCD we actually loose the data.

ETCD is open source,distributed key value data store.
ETCD has consistent Read and Write i.e Any data which we write that is immideately for Read.
ETCD is highly available because data is replicated in all the Nodes.
when we deploy Kubernetes in a production Environment,we normally see the ETCD as external Component and not deployed along with the Kubernetes Cluster.
ETCD is a Cluster of its own.It can be a 3 Node or 5 Node Cluster and maintain a leadership among them using RAFT protocol.
ETCD is secure.

If we set up the Kubernetes cluster from scratch,we can isolate ETCD cluster by creating as a seperate Cluster of its own.
Then advertise the address to the Kubernetes Cluster.
If we set up the Kubernetes cluster using kubeadm,kubeadm deploys all of them as Pods.

To see all the Pods in the Kubernetes system,we use the command

	kubectl get pods --n kube-system

To see all the Configurations in the ETCD Pod we use the command

	kubectl describe pod etcd-minikube -n kube-system

If we run the ETCD as binary or external cluster then we will be able to see the ETCD as Service and runs on port 2379.
All the configuration information will go inside the system d service file.

To intercat with ETCD Cluster or ETCD Pod which is deployed by the minikube local system

	kubectl exec etcd-minikube -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints https// --cacert --key --cert get / -prefix --keys-only"

To insert the data into the ETCD

	kubectl exec etcd-minikube -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints https// --cacert --key --cert put name bhaumik"

	kubectl exec etcd-minikube -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints https// --cacert --key --cert get name"


************************************************************* 6.Kube Controller Manager and Scheduler  ************************************************************

Controller Manager :
--------------------
Controller Manager is another component of Kubernetes echo system that manages the Operational work.
Controller Manager is not a single component and a group of smaller components packaged as a Single Binary.

If we set up the Kubernetes cluster from scratch,download the binary from Kubernetes Release Page and run as a Service.
If we set up the Kubernetes cluster using kubeadm then ControllerManager will be running as Pod.

we have different Controllers in the Controller Manager.
Replication Controller ensures the number of Pods are running as per the Configuration.
Replication Controller keeps monitoring the API Server for any changes.
When particular Pod goes down,kubelet shares the information to the API Server which then shares the information to the ETCD.
Controller Manager gets the information about the Pod from the API Server and 
If the Pod is down then the Controller Manager tries to restart the Pod in the same Node or another Node.

Scheduler :
-----------
Scheduler is another Component of Kubernetes eco system.
Whenever we create a Pod obejct for deployment,Scheduler finds the appropriate Worker Node where the Pod has to placed.

Scheduler has two stages which can be used to deploy the Pod in a correct Node.
Filtering
Scoring

Consider the scenario we have 4 Worker Nodes.

Woker Node 1  : 1 GB
Woker Node 2  : 500 MB
Woker Node 3  : 10 GB
Woker Node 4  : 20 GB

If we want to deploy a Pod which requires 2GB of Memory.
Here Scheduler filters all the worker Nodes and  gets the suitable Nodes.
Scheduler then scores the Nodes based on the available resources and places the Pod into the Node which has the highest score.

To get the Pods in the kube-system namespace we use the command

	kubectl get pods -n kube-system

To describe the configuration of the controller manager

	kubectl describe pod kube-controller-manager-minikube -n kube-system

To describe the configuration of the Scheduler

	kubectl describe pod kube-scheduler-minikube -n kube-system

Note :
We can also create custom scheduler when we craete the Pod.


************************************************************* 7.Pods  **************************************************************************************************

Pod is the smallest deployable unit in Kubernetes.At the End of the day we are going to run Containers in Kubernetes.
Pod is nothing but a collection of Containers i.e Pod hosts the Containers.
All the Containers running inside the Pod shares the same network namespace and storage namespace.
Within the Pods all the Containers can communicate with each other using localhost.
It is the Pod that get's the IP Address to communicate and not the Container.

To run the Pod using imperative command

	kubectl run mypod --image=nginx 
	
To see the list of Pods 

	kubetcl get pods
	
To delete the Pod we use the command

	kubectl delete pod mypod
	
To get the yaml file that can be used to create the Pod

	kubectl run mypod --image=nginx --dry-run=client -o yaml -> nginx.yaml

--dry-run=client flag can be used ignore the run command.
Here Kubernetes will create the yaml manifest file here.
To create the Pod using yaml file 

	kubectl apply -f nginx.yaml
	
Note :
we never run an isolated Pod in Kubernetes environment.
We run the Pod as a part of Deployment,ReplicaSet etc.


Static Pods (Not Completed):
----------------------------
These are the Pods which are created on Worker Nodes managed by Master Node and not created using yaml file.
These are the Pods which are managed by kubelet which is running on the worker Node.

Pod Lifecycle (Not Completed):
------------------------------
Pending	
Running  	 		
Succeded  	

    
************************************************************* 8.Pod Networking  ****************************************************************************************

lets consider how the Docker Networking works.
when we run the Docker Daemon on any Node it internally creates the default docker0 network and all the newly created Containers are attached to the default network.
This default network gets an IP Address.

Whenver we create a Container,Dokcer creates a virtual pair of Ethernet and attaches it to the default Docker Network i.e docker0.
Newly created Container gets virtual pair of Ethernet which has the same range of docker0 Network.
Similar is the case for all the newly created Containers.

When we create the Pod all the Containers within the Pod shares the same Namespace network.They can communicate with each other on the localhost.
Kubernetes implements the shared network Namespace by running the Container named Pause.
Pause Container responsibilty is to hold the same Network Namespace for all the remaining Containers.

In Kubernetes one Pod can reach out to another Pod using IP Address.
Consider the Scenario where the Kubernetes Cluster has 2 Nodes and they are on the Network and communicate with each other either by using Router or API Gateway.
Each Node has a corresponding IP Address.
When we set up the Kubernetes Cluster,Networking PlugIns are not installed and it informs what kind of networking structure it expects.
We need to set up the Networking PlugIns outside the Cluster.
If the Network PlugIns are not installed then there will be no next hop and ends up in creating the same IP for all the Nodes.
There is no mechanism to tell how to send the packet from one Node to another Node.
Here Network PlugIn comes into the picture.

Function of Network PlugIn is to run the Overlay Network across all the Nodes.
Network PlugIn covers all the Bridges which are running on the Node and assigns the IP Address to all the Nodes within the Bridge Network.
Containers running inside the Pod will get the Sequential IP Address.

************************************************************* 9.Static Pods ********************************************************************************************  



   
************************************************************* 10.Container Probes  *************************************************************************************


************************************************************* 11.Replica Sets  *****************************************************************************************

Replica Set is a Kubernetes Controller where it actually maintains the number of Pods as per the Configuration.
Replica Set gaurantees the availability of Pod at any given point of time.
Earlier version of Kubernetes it is called Replication Controller and now it is Replica Set.

Note :
We never run the pods behind the Replica Set directly 
There is high level feature which is called Deployment which provides lots more feature than Replica Set.
Deployment actually creates the Replica Set.
we cannot generate the yaml file for Replica Set using kubectl command like we created the yaml file for pods.

Here we can create a deployment manifest file and run it as a Replica Set.

	kubectl create deployment myreplica --image=nginx --dry-run=client -o yaml >rs.yaml
	
Output of yaml file :

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myreplica
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myreplica
  strategy: {}
  template:
    metadata:
      labels:
        app: myreplica
    spec:
      containers:
      - image: nginx
        name: nginx

To run the deployment we use the command

	kubectl apply -f rs.yaml
	
To view the ReplicaSet

	kubectl get rs
	
To view the Pods

	kubectl get pods

Note :
If we define a pod named myreplica then that pod will be automattically acquired by RepliaSet
and it would be terminated because number of Replica's is already defined in ReplicaSet.


************************************************************* 12.Deployment in Kubernetes   ****************************************************************************
Deployment can be used to deploy the Application.
When we deploy any Application in Kubernetes, we deploy them using Kubernetes.
It is similar to Replica Set with additional features like updating image of an Application,Roll Out's and Roll back of an Application.
To create the deployment manifest file in imperative way we use the commad 

	kubectl create deployment deploy --image=nginx --replicas=1 --dry-run=client -0 yaml > deploy.yaml
	
To run the Deployment we use the command

	kubectl apply -f deploy.yaml
	
When we craete the deployment,Replica Set will be created Automatically.

	kubectl get rs or kubectl get Replica Set
	
To scale the Deployment using imperative command

	kubectl scale deployment deploy --replicas=3
	
Note:
When we perform scaling through imperative command then ther will be no change in yaml file.

To change the image in the deployment we use the command

	kubectl set image deployment/deploy nginx=nginx:1.16.1
	
To view the the updated image in the deployment we use the command

	kubectl describe deployment deploy
	
Note :
If we don't specify any strategy then it is Rolling Update.
To view the rolling update i.e terminates the Pod and creates the New Pod

	watch kubectl get pods
	
If we want to check the Rollout status of the deployment we use the command

	kubectl rollout status deployment deploy
	
To check the rollout history we use the command

	kubectl rollout history deployment deploy
	
If we want to see what happen in revision 3 we use the command

	kubectl rollout history deployment deploy --revision=3
	
To rollback to the previous revision we use the command

	kubectl rollout undo deployment deploy
	
To rollback to the specific revision
	
	kubectl rollout undo deployment deploy --to-revision=3
	

************************************************************* 13.Kubernetes Services Part 1  ***************************************************************************

We have seen how pods communicate with each other using Ip Address.
This is not useful in all the scenario's.

Consider the scenario we have 2 Pods.
Application Pod
Database Pod

	Application Pod  --> Database Pod

If the Application Pod holds the IP Address of the Database Pod and we know that pods in Kubernetes are ephimeral and any time a new pod can come and ha snew Ip Address.
Problem to the solution is we need to use DNS of database instead of Ip Address and any IP Address can be beyond the DNS.
Service in Kubernetes actually provides the proxy i.e behind the service we can put the pods.

	Aplication Pod  -->  Service  <--  Database Pod

Here Service reverse proxy the Database pod and holds the IP Address.
Now the Application can use these IP Address to connect to the Database Pod.

Here IP that service gets is the virtual IP.
The service which we have created does not have any physical or virtual interface and IP Address is not attached to any network interface.

For the communication to be happen between the pods the flow will be
Each node will get the unique Ip Address and there shold be an overlay network between the nodes.
Bridge Network connects to the Ethernet of the Nodes and it internally connects to the Gateway.
Gateway thn communicates with the Ethernet of another Node and which connects to the Bridge Network and the connects to the virtual ethernet of the Pod.

	Application Pod  --> Service   <-- Database Pod
	
When the request goes to the service from Application Pod there is component calle kube-proxy.
kube-proxy maintains all the network configuration rules.
Whenver request originates for the Ip,it redirects the request to the Node which has the running pods or live pods.

Note :
The Ip Address which the Service gets is not from or not in the range of Node Network or Pod Network.
This actually has a different IP Address range.

There are 3 types of services and by default is Cluster IP.

Node Port
Cluster IP
Load Balancer








************************************************************* 14.Kubernetes Service Part 2   ***************************************************************************





************************************************************* 15.Kubernetes Namespace **********************************************************************************




************************************************************* 16.Manual Scheduling    **********************************************************************************   




************************************************************* 17.Kubernetes Affinity and Anti Affinity *****************************************************************