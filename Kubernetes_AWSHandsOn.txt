1.Introduction												[28-MAR-2021]  ()
2.Setup MiniKube                                            [31-MAR-2021]  (Done)
3.Kube API Server                                           [31-MAR-2021]  (Done)
4.ETCD in Kubernetes                                        [01-APR-2021]  (Done)
5.Kubernetes Certified Exam                                 [01-APR-2021]  ()
6.Kube Controller Manager and Scheduler                     [01-APR-2021]  (Done)

7.Pods                                                      [12-APR-2021]  (Done)
8.Pod Networking                                            [12-APR-2021]  (Done)
9.Static Pods                                               [21-APR-2021]  ()
10.Container Probes                                         [21-APR-2021]  ()

11.Replica Sets                                             [03-APR-2021]  ()
12.Deployment in Kubernetes                                 [03-APR-2021]  ()

13.Kubernetes Services Part 1                               [04-APR-2021]  ()
14.Kubernetes Service Part 2                                [04-APR-2021]  ()

15.Kubernetes Namespace                                     [05-APR-2021]  ()
16.Manual Scheduling                                        [05-APR-2021]  ()
17.Kubernetes Affinity and Anti Affinity                    [05-APR-2021]  ()

18.Kubernetes Taints and Tolerants                          [06-APR-2021]  ()
19.ConfigMaps                                               [06-APR-2021]  ()
20.Persistent Volumes and Claims                            [06-APR-2021]  ()

21.ETCD Backup                                              [07-APR-2021]  ()
22.ETCD Database Restore                                    [07-APR-2021]  ()
23.Nginx Ingress Controller                                 [07-APR-2021]  ()



*************************************************************  3.Kube API Server  ***********************************************************************************

API Server is the primary componenet that we intercat with,in the Kubernetes.
When we run the kubectl command,we are actually intercating with API Serevr.
API Server exposes some API that the user can interact with.
we can also intercat with API server using curl Request.

Consider the scenario of creating a Pod and the Steps are 

create the yml file
apply

Here request goes to the API Server and creates the Pod object and updates the ETCD and shows the user that Pod object is created.
The Scheduler which is running in the Kubernetes Cluster continuously monitors the API Server for the New Objects.
Whenever Scheduler finds the new Pod object has created,It looks for the Node where the Pod has to be placed.

Scheduler comes with the information of the available Node and then transfers the information to the API Server.
API Server again updates the ETCD Node with the Node information.
API Server then sends the information to the kubelet which is running on the worker Node.
kubelet gets the Node information from the API Server and communicates with the Container Runtime to create the Pod.
Container Runtime actually creates the Pod and updates the kubelet and then kubelet then updates the API Server that the Pod has created.
API Server then updates the ETCD about the Pod creation.

ETCD is the place where we have complete information about the Cluster.

If we install the Kubernetes as Hardware,download the binary and provision the kube API Server.
If we install the Kubernetes using the kubeadm,kubadm actually runs the API Server as Pod.
When we switch to the console we can actually see how the API Server is running as Pod and we can see all the Configuration details.

To get all the Nodes in the Cluster,we use the command

	kubectl get nodes

To get all the Pods in the Cluster,we use the command

	kubectl get pods

Here no Pods are running as of now.
minikube actually uses the kubeadmin to deploy the Kubernetes cluster.
But we are not able to see all the Kubernetes Components as Pods because all these Pods are deployed in different Namespaces.
All these system Pods runs in a namespace called kube-system.

	kubectl get pods --namespace kube-system (or) kubectl get pods --n kube-system

If we are deploying Kubernetes as a Hardware,download the binary and run the kube API Server as a Service.
Configuration will be in the ETCD  and can view the Configuration in the ETCD service filename.
To see the configuration of the API Server Pod,we use the command

	kubectl describe pod kube-apiserver-minikube -n kube-system

If we want to see the response in Json format

	kubectl get pods -o json

*************************************************************  4.ETCD in Kubernetes  ******************************************************************************

ETCD is the single source of truth for all the components of the Kubernetes System.
ETCD actually stores everything that is happening in the Kubernetes Cluster i.e Pod Creation,Adding Node,Any Deployment etc.
When we do migration in Kubernetes,ETCD is the most importnat Component and If we loose the ETCD we actually loose the data.

ETCD is open source,distributed key value data store.
ETCD has consistent Read and Write i.e Any data which we write that is immideately for Read.
ETCD is highly available because data is replicated in all the Nodes.
when we deploy Kubernetes in a production Environment,we normally see the ETCD as external Component and not deployed along with the Kubernetes Cluster.
ETCD is a Cluster of its own.It can be a 3 Node or 5 Node Cluster and maintain a leadership among them using RAFT protocol.
ETCD is secure.

If we set up the Kubernetes cluster from scratch,we can isolate ETCD cluster by creating as a seperate Cluster of its own.
Then advertise the address to the Kubernetes Cluster.
If we set up the Kubernetes cluster using kubeadm,kubeadm deploys all of them as Pods.

To see all the Pods in the Kubernetes system,we use the command

	kubectl get pods --n kube-system

To see all the COnfigurations in the ETCD Pod we use the command

	kubectl describe pod etcd-minikube -n kube-system

If we run the ETCD as binary or external cluster then we will be able to see the ETCD as Service and runs on port 2379.
All the configuration information will go inside the system d service file.

To intercat with ETCD Cluster or ETCD Node which is deployed by the minikube local system

	kubectl exec etcd-minikube -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints https// --cacert --key --cert get / -prefix --keys-only"

To insert the data into the ETCD

	kubectl exec etcd-minikube -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints https// --cacert --key --cert put name bhaumik"

	kubectl exec etcd-minikube -n kube-system -- sh -c "ETCDCTL_API=3 etcdctl --endpoints https// --cacert --key --cert get name"


************************************************************* 6.Kube Controller Manager and Scheduler  ************************************************************

Controller Manager :
--------------------
Controller Manager is another component of Kubernetes echo system that manages the Operational work.
Controller Manager is not a single component and a group of smaller components packaged as a Single Binary.

If we set up the Kubernetes cluster from scratch,download the binary from Kubernetes Release Pages and run it as Service.
If we set up the Kubernetes cluster using kubeadm then ControllerManager will be running as Pod.

we have different Controller in the Controller Manager.

Replication Controller ensures the number of Pods are running as per the Configuration.
Replication Controller keeps monitoring the API Server fro any changes.
When particular Pod goes down,kubelet shares the information to the API Server which then shares the information to the ETCD.
Controller Manager gets the information about the Pod from the API Server and 
If the Pod is down then the Controlle Manager tries to restart another Pod in the same Node or another Node.

Scheduler :
-----------
Scheduler is another Component of Kubernetes eco system.
Whenever we create a Pod obejct for deployment,Scheduler finds the appropriate Worker Node where the Pod has to placed.

Scheduler has two stages which can be used to deploy the Pod in a correct Node.
Filtering
Scoring

Consider the scenario we have 4 Worker Nodes.

Woker Node 1  : 1 GB
Woker Node 2  : 500 MB
Woker Node 3  : 10 GB
Woker Node 4  : 20 GB

If we want to deploy a Pod which requires 2GB of Memory.
Here Scheduler filters all the worker Nodes and  gets the suitable Nodes.
Scheduler then scores the Nodes based on the available resources and places the Pod into the Node which has the highest score.

To get the Pods in the kube-system namespace,we use the command

	kubectl get pods -n kube-system

To describe the configuration of the controller manager

	kubectl describe pod kube-controller-manager-minikube -n kube-system

To describe the configuration of the Scheduler

	kubectl describe pod kube-scheduler-minikube -n kube-system

Note :
We can also create custome scheduler whenwe craete the Pod.


************************************************************* 7.Pods  **************************************************************************************************

Pod is the smallest deployable unit in Kubernetes.
At the End of the day we are going to run Containers in Kubernetes.
Pod is nothing but a collection of Containers i.e Pod hosts the Containers.
All the Containers running inside the Pod shares the same network namespace and storage namespace.
Within the Pods all the Containers can communicate with each other using localhost.
It is the Pod that get's the IP Address to communicate and not the Container.

To run the Pod using imperative command

	kubectl run mypod --image=nginx 
	
To see the list of Pods 

	kubetcl get pods
	
To delete the Pod we use the command

	kubectl delete pod mypod
	
To get the yaml file that can be used to create the Pod

	kubectl run mypod --image=nginx --dry-run=client -o yaml -> nginx.yaml
	
Here Kubernetes will create the yaml manifest file here.
To create the Pod using yaml file 

	kubectl apply -f nginx.yaml
	
Note :
we never run an isolated Pod in Kubernetes environment.
We run the Pod as a part of Deployment,ReplicaSet etc.


Static Pods (Not Completed):
----------------------------
These are the Pods which are created on Worker Nodes managed by Master Node and not created using yaml file.
These are the Pods which are managed by kubelet which is running on the worker Node.

Pod Lifecycle (Not Completed):
------------------------------
Pending	
Running  	 		
Succeded  	

    
************************************************************* 8.Pod Networking  ****************************************************************************************

lets consider how the Docker Networking works.
when we run the Docker Daemon on any Node it internally creates the default Docker0 network and all the Containers are attached to the default network.
This default network gets an IP Address.

Whenver we create a Container,Dokcer creates a virtual pair of Ethernet and attaches it to the default Docker Network Docker0.
Newly Container gets another virtual Ethernet which has the same range of Docker0 Network.
Similar is the case for all the newly created Containers.

When we create the Pod all the Containers within the Pod shares the same Namespace network.
They can communicate with each other on the localhost.
Kubernetes implements the shared network Namespace by running the Container named Pause.
Pause Container responsibilty is to hold the Network Namespace for all the remaining Containers.

In Kubernetes one Pod can reach out to another Pod using IP Address.
Consider the Scenario where the Kubernetes Cluster has 2 Nodes and they are on the Network and communicate with each other by using Router or API Gateway.
Each Node has a corresponding IP Address.
By default Kubernetes does not implement any networking structure and it informs what kind of networking structure it expects.
When we set up the Kubernetes Cluster Networking PlugIns are not installed.
We need to set up the Networking PlugIns outside the Cluster.
If the Network PlugIns are not installed then there will be no next hop and ends up in creating the same IP for all the Nodes.
There is no mechanism to tell how to send the packet from one Node to another Node.
Here Network PlugIn comes into the picture.

Function of Network PlugIn is to run the Overlay Network across all the Nodes.
Network PlugIn covers all the Bridges which are running on the Node and assigns the IP Address to all the Nodes within the Bridge Network.
Containers running inside the Pod will get the Sequential IP Address.

************************************************************* 9.Static Pods ********************************************************************************************  



   
************************************************************* 10.Container Probes  *************************************************************************************



